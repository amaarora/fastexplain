{
  
    
        "post0": {
            "title": "The Fastai `GetAttr` class",
            "content": "Let&#39;s check the GetAttr source code: . class GetAttr: &quot;Inherit from this to have all attr accesses in `self._xtra` passed down to `self.default`&quot; _default=&#39;default&#39; def _component_attr_filter(self,k): if k.startswith(&#39;__&#39;) or k in (&#39;_xtra&#39;,self._default): return False xtra = getattr(self,&#39;_xtra&#39;,None) return xtra is None or k in xtra def _dir(self): return [k for k in dir(getattr(self,self._default)) if self._component_attr_filter(k)] def __getattr__(self,k): if self._component_attr_filter(k): attr = getattr(self,self._default,None) if attr is not None: return getattr(attr,k) raise AttributeError(k) def __dir__(self): return custom_dir(self,self._dir()) # def __getstate__(self): return self.__dict__ def __setstate__(self,data): self.__dict__.update(data) . The _component_attr_filter checks if the attribute being looked for is not starting with &#39;__&#39; (dunder thingies) or called &#39;_xtra&#39; or self._default. If it&#39;s either of these, return value is False. . Otherwise, if there is some attribute _xtra defined inside the class, then the return is True if k exists in _xtra. OR, return is also True if xtra is None. . Essentially, a call to _component_attr_filter returns True if self._xtra is None or k is part of self._xtra. It returns False if the attribute being looked for is one of dunder thingies or one of self._xtra or self._default. . Finally, when a call to __getattr__ is made, first we check for the attribute self._default if _component_attr_filter returns True. Next, we look for the attribute k being looked inside the self._default attribute thus, passing the getattr to the class&#39;s own attr self._default. . Let&#39;s see an example: . class Robot: name = &quot;IRobot&quot; class Hand(GetAttr): _default = &quot;robot&quot; def __init__(self): self.robot = Robot . hand = Hand() hand.name . &#39;IRobot&#39; . Even the attribute name didn&#39;t exist in Hand class, we still get the value &#39;IRobot&#39; because this class inherits from GetAttr and passes the attribute lookup down to self._default class which in this case is self.robot. . One of the classes that inherits GetAttr is DataLoader and _default is set to dataset which generally is DataSets. . path = untar_data(URLs.MNIST_TINY) dset = Datasets(get_image_files(path)) dl = DataLoader(dset) . Now let&#39;s have a look at dataloaders method. This returns a bound method FilteredBase which actually DataSets inherits from. . As can be seen from the MRO below: . Datasets.__mro__ . (fastai2.data.core.Datasets, fastai2.data.core.FilteredBase, object) . But, this method is made available to DataLoader because _default was set to DataSets and this is due to the magic of GetAttr. . dl.dataloaders . &lt;bound method FilteredBase.dataloaders of (#1428) [(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7565.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7445.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9169.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7924.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7197.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7300.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9024.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9816.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7180.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9515.png&#39;),)...]&gt; .",
            "url": "https://amaarora.github.io/fastexplain/2020/03/31/The-Fastai-GetAttr.html",
            "relUrl": "/2020/03/31/The-Fastai-GetAttr.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Multilabel classification using fastai's `DataBlocks` API",
            "content": "This post is an extension of the previous one on DataBlocks API where we looked at five single image single label classification examples. . In this post, we will look at multilabel classification examples for Computer Vision. This refers to those kinds of problems where a single image can have multiple labels. For example, a challenge could be to recognize common objects present inside an image - in that case the labels would be &quot;desk, table, chair&quot; for a single image with those objects. . As before, we will look at five different multilabel classification examples and build the datablock object for those which is a wrapper that contains - dataloaders, datasets inside fastai. . import fastai2 from fastai2.vision.all import * fastai2.__version__, sys.version . (&#39;0.0.16&#39;, &#39;3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) n[GCC 7.3.0]&#39;) . Example-1: Pascal . As the first example, I would really recommend going over the official documentation here). . Example-2: Freesound Audio Tagging . Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds are difficult to pinpoint. If you close your eyes, could you tell the difference between the sound of a chainsaw and the sound of a blender? . In this example, we will be following @radek&#39;s wonderful notebook on audio classification and apply it to a different dataset. The only difference is unlike Radek&#39;s notebook being a single classification, ours is a multi-label classification - that is, multiple labels can exist for the same audio file. . import librosa import torchaudio import pathlib from IPython.display import Audio . We will be using the curated version Freesound Audio Tagging dataset by Eduardo Fonseca, Manoj Plakal, Frederic Font, Daniel P. W. Ellis, Xavier Serra. Recently, a kaggle competition for the same was also held. . Note that untar_data can be used to get data for any URL not just fastai URLs. . path = untar_data(&quot;https://zenodo.org/record/3612637/files/FSDKaggle2019.audio_train_curated.zip&quot;) . path.ls() . (#2) [Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/labels&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files&#39;)] . def get_audio_files(path): return get_files(path, extensions=&#39;.wav&#39;) . We will be using the fastai&#39;s convenience function get_files that is used get a path of all the files. It is also possible to pass extensions to the function and in that case, the function only returns those files that have extensions that are part of those passed to the method. We will be passing .wav as extension to get all .wav files. . wav_files = get_files(path, extensions=&#39;.wav&#39;); wav_files . (#4970) [Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/ae628bb7.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/f291b5f4.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/0f5c8134.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/749a39f0.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/368e95b1.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/ae8110df.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/0e396c69.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/4bf171d7.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/7730996a.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/8f42bea5.wav&#39;)...] . Audio(filename=str(wav_files[2])) . Your browser does not support the audio element. Can you guys hear traffic noise? Great! :) . The labels can also be downloaded from the same data source, and are present in the train_curated_post_competition.csv file. . labels = pd.read_csv(path/&#39;labels&#39;/&#39;train_curated_post_competition.csv&#39;) labels.set_index(&quot;fname&quot;, inplace=True) labels[&#39;labels&#39;] = labels.labels.str.replace(&quot;,&quot;, &quot; &quot;) labels.head(10) . labels freesound_id license . fname . ac9e7a91.wav Church_bell | 65579 | CC-BY-NC | . 65ae847e.wav Frying_(food) | 65583 | CC Sampling+ | . 32ec2454.wav Computer_keyboard | 360502 | CC0 | . af7b5bab.wav Scissors | 360503 | CC0 | . 7e8cd849.wav Purr | 65598 | CC-BY | . fdfbf113.wav Gasp | 98394 | CC-BY | . 130aa63f.wav Applause Cheering | 32863 | CC-BY-NC | . db198721.wav Applause Crowd Cheering | 32864 | CC-BY-NC | . 492c192f.wav Applause Cheering | 32865 | CC-BY-NC | . 586ab251.wav Applause Cheering | 32866 | CC-BY-NC | . Some of the files such as 492c192f.wav have multiple labels such as Applause and Cheering and therefore this is a multilabel classification problem. . In our get_x we create a spectogram image of the Audio so that we can use a cnn_learner to classify spectograms instead of classifying audio files. This has been a trick that has been used to get state of art results in many audio competitions. . def get_x(path, target_rate=44100, num_samples=int(66150)): x, rate = torchaudio.load_wav(path, normalization=lambda x: torch.abs(x).max()) x = x[0] / 500 x = x.numpy().squeeze() x = librosa.util.fix_length(x, num_samples) spec = librosa.feature.melspectrogram(x, sr=rate, n_fft=1024, hop_length=140) return spec.astype(np.uint8) . For the get_y function we return the labels as a list which will be passed to the MultiCategoryBlock to convert them to ints that represent the category. . def get_y(path, df=labels): return df.loc[path.name][&#39;labels&#39;].split() . We create the DataBlock object as before in the first post. . dblock = DataBlock( blocks=(ImageBlock, MultiCategoryBlock), get_items=get_audio_files, get_x=get_x, get_y=get_y, splitter=RandomSplitter() ) . dls = dblock.dataloaders(path) . As we can see below, the problem now simply becomes an image classification problem. . dls.show_batch() . learn = cnn_learner(dls, resnet34, metrics=[accuracy_multi]) learn.fine_tune(6) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.927335 | 0.562045 | 0.725578 | 01:18 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.624793 | 0.407948 | 0.904867 | 01:20 | . 1 | 0.372609 | 0.118691 | 0.984708 | 01:18 | . 2 | 0.184834 | 0.072461 | 0.985702 | 01:20 | . 3 | 0.114148 | 0.065702 | 0.985978 | 01:22 | . 4 | 0.086914 | 0.062889 | 0.986104 | 01:22 | . 5 | 0.078180 | 0.062337 | 0.986079 | 01:21 | . Very quickly we are able to get to an astonishing multi label accuracy of 98.6%! . Example-3: Atlas Protein Classification . Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells. . For the second example, we will be looking at the Human Protein Atlas Image Classification kaggle competition. Akash Palrecha, Sanyam Bhutani and Rekil Prashanthand I worked together on this example. . The primary challenge with this competition is how the data is presented. The images are four channel and each image is a separate .png file. There are two ways to work in this competition, either concat all four channels together to create a four channel image or select only three channels (randomly or first three), to create a channel image. The below code was run for three channel images - but, just uncomment the &#39;yellow&#39; in concat_four_channel function and replace the cnn_learner with get_model function and this same code will work for four channel images inside fastai. . We will using cv2 to read .png files and concat them together to a three/four channel images. . import cv2 . We use the usual get_image_files to get the .png files and as we can see in the top-4 files, each of the file has id 00070df0-bbc3-11e8-b2bc-ac1f6b6435d0 followed by _blue, _green, _yellow and _red. . path = Path(&quot;/home/ubuntu/repos/kaggle/atlas/data/&quot;) files = get_image_files(path/&#39;train&#39;) files.sort() files[:4] . (#4) [Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_blue.png&#39;),Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_green.png&#39;),Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_red.png&#39;),Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_yellow.png&#39;)] . To create a four channel image, we will have to pass all 4 images to the get_x read each of blue, green, red and yellow images. We do this py passing every 4th image in a sorted list, extract the image_id from the image path passed, and read other three images also. Finally we concat the images and return the four channel images. . def get_4th_item(path): files = get_image_files(path) files.sort() return files[slice(0, len(files), 4)] . The below function concat_4_channel takes in a unique image path, extracts the image_id from the path and reads the other three images as well to create and return a four channel PILImage. . def concat_4_channel(fname): fname = str(fname) if fname.endswith(&#39;.png&#39;) or fname.endswith(&#39;.tif&#39;): suffix = fname[-4:] fname = fname.split(&#39;_&#39;)[0] colors = [&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;, &#39;yellow&#39;] flags = cv2.IMREAD_GRAYSCALE img = [cv2.imread(fname+&#39;_&#39;+color+&#39;.png&#39;, flags).astype(np.uint8) for color in colors] x = np.stack(img, axis=-1) return PILImage.create(x) . Finally, let&#39;s read in the labels. The labels are provided in a separate train.csv file and this problem is a multilabel classification one. A single image can have multiple protein strands. . df = pd.read_csv(path/&quot;train.csv&quot;) df.set_index(&quot;Id&quot;, inplace=True) df.head() . Target . Id . 00070df0-bbc3-11e8-b2bc-ac1f6b6435d0 16 0 | . 000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0 7 1 2 0 | . 000a9596-bbc4-11e8-b2bc-ac1f6b6435d0 5 | . 000c99ba-bba4-11e8-b2b9-ac1f6b6435d0 1 | . 001838f8-bbca-11e8-b2bc-ac1f6b6435d0 18 | . To get the labels inside the DataBlock, we will pass the unique image_id to the dataframe, index into it and get the Target labels. The below function does exactly the same. . def get_labels_from_df(path, df=df): if not isinstance(path, Path): path = Path(path) path = path.stem path = path[:-5] return df.loc[path].values[0].split(&quot; &quot;) . Now we are all set to create the DataBlock object. We pass the get_4th_item in get_items, concat_4_channel as get_x and get_labels_from_df as get_y. . dblock = DataBlock( blocks=(ImageBlock(cls=PILImageBW), MultiCategoryBlock), get_items=get_4th_item, get_x=concat_4_channel, get_y=get_labels_from_df ) . Finally, we call the dataloaders method to create the dataloaders. Remember, the DataBlock is a frame and calling the dataloaders method is when everything is run. . dls = dblock.dataloaders(path/&#39;train/&#39;, bs=32) . Finally, we can call the show_batch function to check the batch and labels. . dls.show_batch() . Excellent, now we are ready to train the learner and make predictions on the test set. . But, we also need to make sure that the model is able to accept four channel images. . def get_model(pretrained=True): m = resnet34(pretrained) wt = m.conv1.weight conv = nn.Conv2d(4,64,7,stride=2,padding=3) weight = conv.weight weight[:,3] = wt.data[:,0].clone() conv.weight = nn.Parameter(weight) m.conv1 = conv return m . Let&#39;s use the competition metric to get a performance estimate of the model. . f1_score = F1ScoreMulti(thresh=0.3) . learn = cnn_learner(dls, resnet34, metrics=[f1_score]) # learn = cnn_learner(dls, get_model, metrics=[f1_score]) . Great, now let&#39;s learn the learning rate finder to get an appropriate learning rate. . learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.04786301031708717) . learn.fine_tune(15, base_lr=1e-3) . epoch train_loss valid_loss f1_score time . 0 | 0.175298 | 0.147654 | 0.130897 | 02:14 | . epoch train_loss valid_loss f1_score time . 0 | 0.145895 | 0.129687 | 0.217130 | 03:03 | . 1 | 0.131300 | 0.117884 | 0.281944 | 03:03 | . 2 | 0.119659 | 0.108711 | 0.339926 | 03:03 | . 3 | 0.103923 | 0.103715 | 0.387413 | 03:03 | . 4 | 0.092060 | 0.102526 | 0.423547 | 03:03 | . 5 | 0.073156 | 0.105685 | 0.440281 | 03:03 | . 6 | 0.053137 | 0.115068 | 0.435095 | 03:03 | . 7 | 0.036532 | 0.125479 | 0.442718 | 03:03 | . 8 | 0.024828 | 0.137905 | 0.454886 | 03:03 | . 9 | 0.017110 | 0.143491 | 0.462277 | 03:03 | . 10 | 0.012493 | 0.148489 | 0.445136 | 03:03 | . 11 | 0.008337 | 0.150278 | 0.459201 | 03:03 | . 12 | 0.006240 | 0.154228 | 0.455904 | 03:03 | . 13 | 0.005097 | 0.154765 | 0.456672 | 03:03 | . 14 | 0.004447 | 0.154457 | 0.453888 | 03:03 | . learn.fine_tune(5, base_lr=2e-2) . epoch train_loss valid_loss f1_score time . 0 | 0.046586 | 0.218287 | 0.386139 | 02:16 | . epoch train_loss valid_loss f1_score time . 0 | 0.117914 | 0.115358 | 0.349613 | 03:03 | . 1 | 0.110822 | 0.100517 | 0.425270 | 03:03 | . 2 | 0.094380 | 0.092146 | 0.456394 | 03:03 | . 3 | 0.069953 | 0.084606 | 0.514543 | 03:03 | . 4 | 0.048090 | 0.087329 | 0.536810 | 03:03 | . learn.save(&quot;model.bin&quot;) . test_files = get_4th_item(path/&quot;test&quot;) . dl = learn.dls.test_dl(test_files, bs=64) . preds, _ = learn.get_preds(dl=dl) . thresh = 0.3 labelled_preds = [&#39; &#39;.join([learn.dls.vocab[i] for i,p in enumerate(pred) if p &gt; thresh]) for pred in preds.numpy()] . sub = pd.DataFrame({ &#39;Id&#39; : [item[:-5] for item in test_files.attrgot(&quot;stem&quot;)], &#39;Predicted&#39;: labelled_preds }) . sub.head() . Id Predicted . 0 00008af0-bad0-11e8-b2b8-ac1f6b6435d0 | 2 | . 1 0000a892-bacf-11e8-b2b8-ac1f6b6435d0 | 5 | . 2 0006faa6-bac7-11e8-b2b7-ac1f6b6435d0 | 0 25 5 | . 3 0008baca-bad7-11e8-b2b9-ac1f6b6435d0 | 0 25 | . 4 000cce7e-bad4-11e8-b2b8-ac1f6b6435d0 | 23 | . sub.to_csv(&quot;submission.csv&quot;, index=False) . !kaggle competitions submit -c human-protein-atlas-image-classification -f submission.csv -m &quot;fastai basic&quot; . 100%|████████████████████████████████████████| 472k/472k [00:09&lt;00:00, 50.5kB/s] Successfully submitted to Human Protein Atlas Image Classification . Conclusion . In this blogpost, we have had a look at how to get multilabel predictions from the fastai library outside the official example of Pascal. The first example followed @radek&#39;s wonderful notebook on audio classification and we applied it to a different dataset. . Next, inside the second example, we had a look at the Protein classification problem and also saw how to read four channel images inside the library. . Credits . Special thanks to Radek Osmulski for the wonderful Audio classification notebook and sharing it with everyone on the Fastai Forums. . Also, I want to thank Akash Palrecha, Sanyam Bhutani and Rekil Prashanth for working on the second example together. .",
            "url": "https://amaarora.github.io/fastexplain/2020/03/26/DataBlocks-API-multi-label-classification.html",
            "relUrl": "/2020/03/26/DataBlocks-API-multi-label-classification.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "DataBlocks API - A high level introduction with examples",
            "content": "Fastai&#39;s very popular Part-1 Deep Learning for coders course has begun this week from March 17, 2020. I am honored to be a part of the group that get&#39;s to do it &quot;live&quot; before the course get&#39;s released to the general public in July. . A high level introduction to the DataBlocks can be found here while the official DataBlocks documentation can be referenced here. . In general the DataBlocks API is an easy-to-use, highly flexible and a very powerful API that can be used to build DataLoaders and Datasets for various different Deep Learning applications such as VISION, TEXT, TABULAR and COLLABORATION. . In this post, we will be focussing on VISION applications and learn to use the DataBlocks API for 5 different vision based single label deep learning applications. . We will be following the top-down approach as in Fastai and learn about the DataBlocks API by &quot;doing rather than reading&quot;. . We will be using Kaggle for 5 different single label CV applications (each one a little bit different from the other) and build a DataBlock for each of these competitions. In this post the focus will be on getting the data in, rather than model training. . In the world of deep learning, getting the data ready for training is an essential (usually time consuming) step and we will be focusing on this rather than model training in this blog post by making use of the DataBlocks API. . # using version 0.0.14 of fastai2 import fastai2 from fastai2.vision.all import * fastai2.__version__, sys.version . (&#39;0.0.14&#39;, &#39;3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) n[GCC 7.3.0]&#39;) . Example-1: MNIST . The dataset for this example consists of image files that are split by folders. Example referenced from the fastai documentation here. . Let&#39;s start with the hello-world example of deep learning for handwritten digit classification. We will be building the DataBlocks API for a smaller version of this dataset but this will also hold for the complete version. . path = untar_data(URLs.MNIST_TINY) . # !sudo apt-get install tree !tree -d {path} . /home/ubuntu/.fastai/data/mnist_tiny ├── models ├── test ├── train │   ├── 3 │   └── 7 └── valid ├── 3 └── 7 8 directories . As we can see from the folder structure above, we have train, test and valid folders. Each of train and valid folders have 3 and 7 subfolders with images while test consists of some image files. . files = get_image_files(path) . get_image_files is a convenience function that returns the paths of all image files that are available inside the path tree passed as an argument. . files . (#1428) [Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7565.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7445.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9169.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7924.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7197.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7300.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9024.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9816.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7180.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9515.png&#39;)...] . As a general concept, we need to tell the DataBlock API 4 things: . What kind of problem we&#39;re working on? | How to get the items? | How to label the items? | How to create the validation set? | dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=GrandparentSplitter()) . As a DataBlock introduction, the four things that we pass to build our DataBlock object are: . blocks: Generally, two blocks are always passed for each different example - one, for the X and another for the y variable. Since, in this case, our Xs are images, the X block becomes ImageBlock and since this is single category classification, our y block becomes CategoryBlock. The blocks share the information regarding the type of problem we&#39;re working on. . get_items: Next, we need to pass a function to the API that get&#39;s applied on the source passed to the dblock.dataloaders(source) method. This function tells the DataBlock API on how to get the items. . files = get_image_files(path); files . (#1428) [Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7565.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7445.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9169.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7924.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7197.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7300.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9024.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9816.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7180.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9515.png&#39;)...] . As can be seen from above example, get_image_files returns a L (similar but more powerful than the Python List in fastai) of items which are Paths to various image files. . get_y: The get_y function get&#39;s applied to each of the files returned after get_items is applied to source. This function returns the y variable from these paths. In our case, we pass the parent_label which returns the parent label of a function (the folder name) which becomes the y variable. This works because of the way our data is set up. . splitter: The last thing that we pass to the DataBlocks API is a splitter. The splitter get&#39;s applied to each of the items returned by get_items function to split the data into train and valid set. Since, our data is already split into train and valid folders, we pass GrandParentSplitter as the splitter function which splits the data into train and valid based on grand parent folder name. . dls = dblock.dataloaders(path) . dls.train_ds[0] . (PILImage mode=RGB size=28x28, TensorCategory(1)) . dls.show_batch() . Example-2: PETS . All images for the PETS dataset exist in a single directory but the labels can be extracted fro filenames themselves. Example referenced from the fastai documentation here. . path = untar_data(URLs.PETS)/&#39;images&#39;; path . Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images&#39;) . # !sudo apt-get install tree !tree -d {path} . /home/ubuntu/.fastai/data/oxford-iiit-pet/images 0 directories . Since, we have 0 directories, it means all image files are part of this one directory /home/ubuntu/.fastai/data/oxford-iiit-pet/images. This path might be different on your machine.. . Also, the y labels need to be extracted from the image names themselves. This is different from the folder structure that we saw in MNIST. . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(224,224)), get_items=get_image_files, get_y=[attrgetter(&quot;name&quot;), RegexLabeller(pat=r&#39;^(.*)_ d+.jpg$&#39;)], splitter=RandomSplitter()) . Because the objective here is to do single label classification again, therefore, we pass the ImageBlock and CategoryBlock as the X and y block again. . We also pass item_tfms which randomly crops and resizes each of the images to a 224*224 size. We need this for model training - in deep learning, we require that our image sizes are the same. As the name suggests item_tfms get applied to each X item, therefore, each image is randomly cropped and resized to 224*224. . Our get_items function is the same as before. . For get_y since, we need to extract images from the Path, first we get the name attribute of each of the paths returned by get_items, next we use a RegexLabeller to extract the label from the names. . files = get_image_files(path) files[:10] . (#10) [Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg&#39;)] . names = files[:10].attrgot(&quot;name&quot;) names . (#10) [&#39;keeshond_34.jpg&#39;,&#39;Siamese_178.jpg&#39;,&#39;german_shorthaired_94.jpg&#39;,&#39;Abyssinian_92.jpg&#39;,&#39;basset_hound_111.jpg&#39;,&#39;Russian_Blue_194.jpg&#39;,&#39;staffordshire_bull_terrier_91.jpg&#39;,&#39;Persian_69.jpg&#39;,&#39;english_setter_33.jpg&#39;,&#39;Russian_Blue_155.jpg&#39;] . labeller = RegexLabeller(pat=r&#39;^(.*)_ d+.jpg$&#39;) labels = names.map(labeller) labels . (#10) [&#39;keeshond&#39;,&#39;Siamese&#39;,&#39;german_shorthaired&#39;,&#39;Abyssinian&#39;,&#39;basset_hound&#39;,&#39;Russian_Blue&#39;,&#39;staffordshire_bull_terrier&#39;,&#39;Persian&#39;,&#39;english_setter&#39;,&#39;Russian_Blue&#39;] . The above three steps explain how we extract labels from our paths that were returned by the get_items function. . dls = dblock.dataloaders(path) . dls.show_batch() . Example-3: Google Landmark . Google Landmark Recognition 2019 kaggle competition to predict the landmark shown in the image. . Since the complete dataset size for this competition is around 100 gb, we will only be working on a subset of the dataset. . path = download_data(&quot;https://s3.amazonaws.com/google-landmark/train/images_000.tar&quot;) path . Path(&#39;/home/ubuntu/.fastai/archive/images_000.tar&#39;) . So the .tar file get&#39;s downloaded at home/ubuntu/.fastai/archive/images_000.tar at my machine. download_data is a convenience function inside fastai and more about it can be found here. . #untar file !tar -xf {path} . !tree -d {path.parent/&#39;0&#39;} . /home/ubuntu/.fastai/archive/0 └── 0 ├── 0 ├── 1 ├── 2 ├── 3 ├── 4 ├── 5 ├── 6 ├── 7 └── 8 10 directories . In this competition, all images are present in the path (similar to PETS) but the labels are provided in a .csv file. This is different from PETS where we built the labels from the path names themselves. . Let&#39;s download that file. From the competition information available here the train .csv file can be downloaded from https://s3.amazonaws.com/google-landmark/metadata/train.csv. . trn_csv_pth = download_data(&quot;https://s3.amazonaws.com/google-landmark/metadata/train.csv&quot;) trn_csv_pth . Path(&#39;/home/ubuntu/.fastai/archive/train.csv&#39;) . trn_labels = pd.read_csv(trn_csv_pth) trn_labels.head() . id url landmark_id . 0 6e158a47eb2ca3f6 | https://upload.wikimedia.org/wikipedia/commons/b/b5/Observatoriet_v%C3%A4derkammer_2013a.jpg | 142820 | . 1 202cd79556f30760 | http://upload.wikimedia.org/wikipedia/commons/6/63/Ecosse200996-1.jpg | 104169 | . 2 3ad87684c99c06e1 | http://upload.wikimedia.org/wikipedia/commons/2/2c/Pirmasens_Dynamikum.jpg | 37914 | . 3 e7f70e9c61e66af3 | https://upload.wikimedia.org/wikipedia/commons/0/02/Occidental_Vertical.jpg | 102140 | . 4 4072182eddd0100e | https://upload.wikimedia.org/wikipedia/commons/5/51/Looking_downstream_from_the_footbridge_over_the_Severn_-_geograph.org.uk_-_532337.jpg | 2474 | . Since we don&#39;t need the url column, let&#39;s drop it. . trn_labels.drop(&quot;url&quot;, axis=1, inplace=True) . So in this competition, the images are named as id.jpg from the dataframe above and the labels are inside the df as well. This is different from both the PETS and MNIST data structure. . files = get_image_files(path.parent/&#39;0&#39;) . files . (#8266) [Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/00562e33f481adc5.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0054a9a95ff4190c.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0054318893cd2ea8.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0059b37f9ea1e443.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/00509f87e2140fe8.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0059cc88f2dbb53a.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/005303e80b4f14a2.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0050051bd1831856.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/00575e870647b87c.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/005959682da86fa8.jpg&#39;)...] . So now we have a list of 8266 image files and since this is only a subset of the dataset, we need to only keep those rows in the trn_labels dataframe for which we have images. The following step wouldn&#39;t be necessary if you&#39;re working with the complete dataset. . fnames = files.attrgot(&#39;stem&#39;) fnames . (#8266) [&#39;00562e33f481adc5&#39;,&#39;0054a9a95ff4190c&#39;,&#39;0054318893cd2ea8&#39;,&#39;0059b37f9ea1e443&#39;,&#39;00509f87e2140fe8&#39;,&#39;0059cc88f2dbb53a&#39;,&#39;005303e80b4f14a2&#39;,&#39;0050051bd1831856&#39;,&#39;00575e870647b87c&#39;,&#39;005959682da86fa8&#39;...] . trn_labels = trn_labels[trn_labels[&quot;id&quot;].isin(fnames)].reset_index(drop=True) trn_labels.set_index(&quot;id&quot;, inplace=True) trn_labels.head() . landmark_id . id . 0036d78c05c194d9 50089 | . 001cd787f1e9a803 61937 | . 00429b0a692bc6ec 183170 | . 0082fd4214b3c2c7 36407 | . 002b386016930458 119649 | . Now we are ready to create our DataBlock object. . def get_label_from_df(path, df=trn_labels): id = path.stem return df.loc[id].values[0] . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms = RandomResizedCrop(size=(224,224)), get_items=get_image_files, get_y=get_label_from_df, splitter=RandomSplitter()) . dls = dblock.dataloaders(path.parent/&#39;0&#39;) . dls.show_batch() . Example-4: Distracted Driver Detection . In this competition you are given driver images, each taken in a car with a driver doing something in the car (texting, eating, talking on the phone, makeup, reaching behind, etc). Your goal is to predict the likelihood of what the driver is doing in each picture. . data_path = Path(&quot;/home/ubuntu/repos/kaggle/driver/data/&quot;) . !tree -d {data_path} . /home/ubuntu/repos/kaggle/driver/data └── imgs ├── test └── train ├── c0 ├── c1 ├── c2 ├── c3 ├── c4 ├── c5 ├── c6 ├── c7 ├── c8 └── c9 13 directories . All images are split into folders similar to the PETS data structure. But, for this type of competition, we won&#39;t be using the DataBlocks API similar to PETS. The reason wil be explained in a moment. . We will use the same get_image_files as before, to read in the files from the data_path. . files = get_image_files(data_path/&quot;imgs&quot;) . files . (#102150) [Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_31511.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_63076.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_27600.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_31852.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_98339.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_68938.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_85715.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_41593.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_78475.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_99303.jpg&#39;)...] . Also, an interesting thing here is that we have around ~20K train images while we have ~80K test images. . Similar to the Google Landmark Images competition from before, the labels are given in a dataframe while the images are available in the train folder. We will build the DataBlocks API a little differently than we did in the last competition. This time we will instead use the pandas dataframe, to get_x and get_y labels. . We use the dataframe instead of splitting by folders, because with the dataframe we can pass the valid_idx values (index values for the validation set). From the Kaggle competition here we can see that the test set consists of driver images of those drivers that were not part of the train set. . Therefore, we would like our validation set to consist of drivers not part of the train set as well. This is possible by passing in validation index values such that subject in the train dataframe are different in validation set from those in train set. . trn_labels = pd.read_csv(data_path/&quot;driver_imgs_list.csv&quot;) trn_labels.head() . subject classname img . 0 p002 | c0 | img_44733.jpg | . 1 p002 | c0 | img_72999.jpg | . 2 p002 | c0 | img_25094.jpg | . 3 p002 | c0 | img_69092.jpg | . 4 p002 | c0 | img_92629.jpg | . def get_paths_from_df(df_values): path = Path(data_path)/&quot;imgs&quot;/&quot;train&quot;/df_values[1]/df_values[2] return PILImage.create(path) . In this competition, the idea is to predict the classname rather than the subject so we will use this as our y_label. . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(224,224)), get_x=get_paths_from_df, splitter=IndexSplitter(trn_labels[-1983:].index), get_y=ColReader(1) ) . The DataBlock API is built differently than the Google Landmark Detection challenge where we used a RandomSplitter, got our Xs from paths, while ys from dataframe. . Instead, in this competition, we use an IndexSplitter to make sure that the valid set consists of drivers not part of train set. . We also use get_paths_from_df as our get_x. Essentially, all that this function does is to generate a path of our image based on the trn_labels dataframe values. Once we get the path from get_x, we use PILImage.create method to return a PIL Image. This image becomes our X. . To get the y variable we simply read the &quot;classname&quot; column of the dataframe. . dls = dblock.dataloaders(trn_labels.values) . dls.show_batch() . Now, getting these c0, c1 class names are not very helpful. Let&#39;s instead get the actual image captions. We can do this by simply adding a get_true_label function which returns the true label based on a dictionary defined in the Kaggle competition. . def get_true_label(key): return dict( c0= &#39;safe driving&#39;, c1= &#39;texting - right&#39;, c2= &#39;talking on the phone - right&#39;, c3= &#39;texting - left&#39;, c4= &#39;talking on the phone - left&#39;, c5= &#39;operating the radio&#39;, c6= &#39;drinking&#39;, c7= &#39;reaching behind&#39;, c8= &#39;hair and makeup&#39;, c9= &#39;talking to passenger&#39; )[key] . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(256, 256)), get_x=get_paths_from_df, splitter=IndexSplitter(trn_labels[-1983:].index), get_y=[ColReader(1), get_true_label] ) . We add the get_true_label function to the get_y and now the get_y becomes a list of functions which will be applied serially. The first function ColReader(1) will read the second column and return the &quot;classname&quot; such as c0, c1.. and next, get_true_label will return a true label based on the dictionary and key. The key passed to get_true_label is the output of ColReader(1) therefore, one of c0, c1, c2.. . Let&#39;s build the DataBlock object again and check the dataloader batch. . dls = dblock.dataloaders(trn_labels.values) . dls.show_batch() . Example-5: Plant Pathology 2020 - FGVC7 . Given a photo of an apple leaf, can you accurately assess its health? This competition will challenge you to distinguish between leaves which are healthy, those which are infected with apple rust, those that have apple scab, and those with more than one disease. . data_path = Path(&quot;/home/ubuntu/repos/kaggle/plant/data/&quot;) . !tree -d {data_path} . /home/ubuntu/repos/kaggle/plant/data └── images 1 directory . All images are inside the images directory. Let&#39;s use the get_image_files function to have a look at them. . files = get_image_files(data_path/&#39;images&#39;) files . (#3642) [Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_116.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_542.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_798.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Train_159.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_61.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_713.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_1387.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Train_556.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_551.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Train_499.jpg&#39;)...] . Each label path name consists of Test or Train labels (which we can use for splitting). The Test files do not have any labels because this the test set on which we need to submit predictions to Kaggle. . Therefore, we can use the Train set to read images, do a random split using the RandomSpliter() to create a random valid dataset and finally run predictions on the test set. . trn_labels = pd.read_csv(data_path/&quot;train.csv&quot;) . image_id healthy multiple_diseases rust scab . 0 Train_0 | 0 | 0 | 0 | 1 | . 1 Train_1 | 0 | 1 | 0 | 0 | . 2 Train_2 | 1 | 0 | 0 | 0 | . 3 Train_3 | 0 | 0 | 1 | 0 | . 4 Train_4 | 1 | 0 | 0 | 0 | . Also, the labels are one-hot encoded inside the DataFrame. Therefore, to make the DataBlocks API simpler, I believe it&#39;s best to convert the one-hot-encoded variables to integers. . cats = [np.where(r==1)[0][0] for r in trn_labels[[&#39;healthy&#39;, &#39;multiple_diseases&#39;, &#39;rust&#39;, &#39;scab&#39;]].values] trn_labels[&#39;cat&#39;] = cats . trn_labels.head() . image_id healthy multiple_diseases rust scab cat . 0 Train_0 | 0 | 0 | 0 | 1 | 3 | . 1 Train_1 | 0 | 1 | 0 | 0 | 1 | . 2 Train_2 | 1 | 0 | 0 | 0 | 0 | . 3 Train_3 | 0 | 0 | 1 | 0 | 2 | . 4 Train_4 | 1 | 0 | 0 | 0 | 0 | . def get_true_label(key): return { 0: &#39;healthy&#39;, 1: &#39;multiple_diseases&#39;, 2: &#39;rust&#39;, 3: &#39;scab&#39; }[key] . The get_x function reader the first column of the trn_labels dataframe above, adds a &quot;data_path/images/&quot; prefix to it and a .jpg suffix, therefore, converting it to a path. . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(224, 224)), get_x=ColReader(0, pref=f&quot;{data_path}/images/&quot;, suff=&quot;.jpg&quot;), splitter=RandomSplitter(), get_y=[ColReader(5), get_true_label] ) . We call the dataloaders as before to create the dataloaders. . dls = dblock.dataloaders(trn_labels.values) . dls.show_batch() . Summary . So, in this post, we specifically looked at single image, single label type Vision Classification problems and looked at different ways of using the DataBlocks API to build the DataBlock object and dataloaders. . In the coming few days, we will look at the DataBlocks API in more detail, use the same API structure to build dataloaders for Single Image, Multi Label type classification, text classification and also look at an interesting example on how to deal with Multi Image, Single Label classification type problem. . In the future posts, we will also start digging deeper in to the implementation details of the DataBlocks API and the fastai library as a whole. . Credits . Thanks to @vijayabhaskar for pointing out that it would be easier to build the DataBlocks API in the fifth example using ColReader in get_x. | Thanks to @muellerzr for pointing out a typo in the blog. |",
            "url": "https://amaarora.github.io/fastexplain/2020/03/24/DataBlocks-API.html",
            "relUrl": "/2020/03/24/DataBlocks-API.html",
            "date": " • Mar 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Data Scientist at CoreLogic Australia where I spend a lot of my time building SOTA algorithms to predict house and rental prices across all of Australia and NZ. At CoreLogic, I have been responsible for building the Rental AVM product from scratch for over 10 million properties which led to an overall 8% increase in predictions nationally. . I have a passion for deep learning, thanks to Jeremy Howard who made it possible for me through fast.ai to begin my journey in this space around 1.5 years ago. . I am self taught, and an ardent follower of and small contributor to the fast.ai course. Through this course itself, I have been able to pickup Python and it has made possible for me to do my own research. Something, I would have not thought of being able to do when I first started. I believe I still have a lot to learn and commit myself to learning something new every day. . I also fimly believe in giving back to the community and in that attempt, also run a meetup group called Applied Data Science. The focus in this blog is on writing code as much as it is on explaining the concept in theory. . If you feel that any of these ideas can be expressed in a better way, I am very happy to receive constructive feedback. Please feel free to reach out to me via twitter. .",
          "url": "https://amaarora.github.io/fastexplain/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://amaarora.github.io/fastexplain/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}