{
  
    
        "post0": {
            "title": "",
            "content": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks . EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks Introduction | The “WHY”? | The “HOW”? Compound Scaling | Nueral Architecture Search | Main Contributions - CS &amp; NAS | | Comparing Conventional Methods with Compound Scaling Depth | Width | Resolution | Compound Scaling | | The EfficientNet Architecture using NAS MnasNet Approach | Nueral Architecture Search for EfficientNets | Inverted Bottleneck MBConv | | What have we learnt so far | | Introduction . It brings me great pleasure as I begin writing about EfficientNets for three reasons: . At the time of writing, Fixing the train-test resolution discrepancy: FixEfficientNet (family of EfficientNet) is the current State of Art on ImageNet with 88.5% top-1 accuracy and 98.7% top-5 accuracy. | As far as I am aware, this is the only blog that explains the EfficientNet Architecture in detail along with code implementations. | This blog post also sets up the base for future blog posts on Self-training with Noisy Student improves ImageNet classification, Fixing the train-test resolution discrepancy and Fixing the train-test resolution discrepancy: FixEfficientNet. | In this blog post, in The “Why” section, we take a look at the superior performance of EfficientNets compared to their counterparts and understand why EfficientNets are totally worth your time. . Next, in “The How” section, we start to unravel the magic inside EfficientNets. Particularly, we look at two main contributions from the research paper: . Compound Scaling | The EfficientNet Architecture (developed using Nueral Architecture Search) | Having introduced the two contributions in The “How”, we the compare the conventional methods of scaling with Compound Scaling approach in Comparing Conventional Methods with Compound Scaling. . Finally we look at the details of the EfficientNet Architecture in The EfficientNet Architecture using NAS and also look at code level implementations in Code overview in PyTorch. . So, let’s get started! . The “WHY”? . In this section we understand “why” EfficientNets are totally worth your time. . fig-1 below summarizes “why” we could a learn a lot by understanding the EfficientNet Architecture. . . As we can see from fig-1, EfficientNets significantly outperform other ConvNets. In fact, EfficientNet-B7 achieved new state of art with 84.4% top-1 accuracy outperforming the previous SOTA GPipe but being 8.4 times smaller and 6.1 times faster. . From the paper, . EfficientNet-B7 achieves state- of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. . The great thing about EfficientNets is that not only do they have better accuracies compared to their counterparts, they are also lightweight and thus, faster to run. . Having looked at their superior accuracies and faster runtimes, let’s start to unravel the magic step-by-step. . The “HOW”? . So “how” did the authors Mingxing Tan and Quoc V. Le make EfficientNets perform so well and efficiently? . In this section we will understand the main idea introduced in the research paper - Compound Scaling. . Compound Scaling . Before the EfficientNets came along, the most common way to scale up ConvNets was either by one of three dimensions - depth (number of layers), width (number of channels) or image resolution (image size). . EfficientNets on the other hand perform Compound Scaling - that is, scale all three dimensions while mantaining a balance between all dimensions of the network. . From the paper: . In this paper, we want to study and rethink the process of scaling up ConvNets. In particular, we investigate the central question: is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency? Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. Based on this observation, we propose a simple yet effective compound scaling method. Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. . This main difference between the scaling methods has also been illustrated in fig-2 below. . . In fig-2 above, (b)-(d) are conventional scaling that only increases one dimension of network width, depth, or resolution. (e) is the proposed compound scaling method that uniformly scales all three dimensions with a fixed ratio. . This main idea of Compound Scaling really set apart EfficientNets from its predecessors. And intuitively, this idea of compound scaling makes sense too because if the input image is bigger (input resolution), then the network needs more layers (depth) and more channels (width) to capture more fine-grained patterns on the bigger image. . In fact, this idea of Compound Scaling also works on existing MobileNet and ResNet architectures. . From table-1 below, we can clearly see, that the versions of MobileNet and ResNet architectures scaled using the Compound Scaling approach perform better than their baselines or also those architectures that were scaled using conventional methods - (b)-(d) in fig-2. . . Thus, it is safe to summarize - Compound Scaling works! But, we’re not done yet, there’s more magic to be unraveled. . Nueral Architecture Search . Since we are looking at the “how” - while so far we know Compound Scaling was the main idea introduced - the authors found that having a good baseline network is also critical. . It wasn’t enough to achieve such great performance by picking up any existing architecture and applying Compound Scaling to it. While the authors evaluated the scaling method using existing ConvNets (for example - ResNets and MobileNets in table-1 before), in order to better demonstrate the effectiveness of this scaling method, they also developed a new mobile-size baseline, called EfficientNet using Nerual Architecture Search. . We understand how they did this is in a lot more detail in a later section of this blog post. . Main Contributions - CS &amp; NAS . Therefore, to summarize the two main contributions of this research paper were the idea of Compound Scaling and using Nueral Architecture Search to define a new mobile-size baseline called EfficientNet. We look at both model scaling and the EfficientNet architecture in a lot more detail in the following sections. . Comparing Conventional Methods with Compound Scaling . In this section we look at various ways of scaling nueral networks in a lot more detail and compare then with the Compound Scaling approach. . Basically, the authors of EfficientNet architecture ran a lot of experiments scaling depth, width and image resolution and made two main observations: . Scaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models. | In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling. | . These two observations can also be seen in fig-3. Now, let’s look at the effects of scaling single dimensions on a ConvNet in more detail below. . Depth . Scaling network depth (number of layers), is the most common way used by many ConvNets. . With the advancements in deep learning (particularly thanks to Residual Connections, BatchNorm), it has now been possible to train deeper nueral networks that generally have higher accuracy than their shallower counterparts. The intuition is that deeper ConvNet can capture richer and more complex features, and generalize well on new tasks. However, deeper networks are also more difficult to train due to the vanishing gradient problem. Although residual connections and batchnorm help alleviate this problem, the accuracy gain of very deep networks diminishes. For example, ResNet-1000 has similar accuracy as ResNet-101 even though it has much more layers. . In fig-3 (middle), we can also see that ImageNet Top-1 Accuracy saturates at d=6.0 and no further improvement can be seen after. . Width . Scaling network width - that is, increasing the number of channels in Convolution layers - is most commonly used for smaller sized models. We have seen applications of wider networks previously in MobileNets, MNasNet. . While wider networks tend to be able to capture more fine-grained features and are easier to train, extremely wide but shallow networks tend to have difficul- ties in capturing higher level features. . Also, as can be seen in fig-3 (left), accuracy quickly saturates when networks become much wider with larger w. . Resolution . From the paper: . With higher resolution input images, ConvNets can potentially capture more fine-grained patterns. Starting from 224x224 in early ConvNets, modern ConvNets tend to use 299x299 (Szegedy et al., 2016) or 331x331 (Zoph et al., 2018) for better accuracy. Recently, GPipe (Huang et al., 2018) achieves state-of-the-art ImageNet accuracy with 480x480 resolution. Higher resolutions, such as 600x600, are also widely used in object detection ConvNets (He et al., 2017; Lin et al., 2017). . Increasing image resolution to help improve the accuracy of ConvNets is not new - This has been termed as Progressive Resizing in fast.ai course. (explained here). . It is also beneficial to ensemble models trained on different input resolution as explained by Chris Deotte here. . fig-3 (right), we can see that accuracy increases with an increase in input image size. . By studying the indivdiual effects of scaling depth, width and resolution, this brings us to the first observation which I post here again for reference: . Scaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models. . Compound Scaling . . Each dot in a line in fig-4 above denotes a model with different width(w). We can see that the best accuracy gains can be obvserved by increasing depth, resolution and width. r=1.0 represents 224x224 resolution whereas r=1.3 represents 299x299 resolution. . Therefore, with deeper (d=2.0) and higher resolution (r=2.0), width scaling achieves much better accuracy under the same FLOPS cost. . This brings to the second observation: . In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling. . Having looked at Compound Scaling, we will now look at how the authors used Nueral Architecture Search to get mobile-size network that they named EfficientNet. . The EfficientNet Architecture using NAS . The authors used Nueral Architecture Search approach similar to MNasNet research paper. This is a reinforcement learning based approach where the authors developed a baseline neural architecture Efficient-B0 by leveraging a multi-objective search that optimizes for both Accuracy and FLOPS. From the paper: . Specifically, we use the same search space as (Tan et al., 2019), and use ACC(m)×[FLOPS(m)/T]w as the optimization goal, where ACC(m) and FLOPS(m) denote the accuracy and FLOPS of model m, T is the target FLOPS and w=-0.07 is a hyperparameter for controlling the trade-off between accuracy and FLOPS. Unlike (Tan et al., 2019; Cai et al., 2019), here we optimize FLOPS rather than latency since we are not targeting any specific hardware device. . The EfficientNet-B0 architecture has been summarized in table-2 below: . . The MBConv layer above is nothing but an inverted bottleneck block with squeeze and excitation connection added to it. We will learn more about this layer in this section of the blog post. . Starting from this baseline architecture, the authors scaled the EfficientNet-B0 using Compound Scaling to obtain EfficientNet B1-B7. . MnasNet Approach . Before we understand how was the EfficientNet-B0 architecture developed, let’s first look into the MnasNet Architecture and the main idea behind the research paper. . . From the paper: . The search framework consists of three components: a recurrent neural network (RNN) based controller, a trainer to obtain the model accuracy, and a mobile phone based inference engine for measuring the latency. . For MNasNet, the authors used model accuracy (on ImageNet) and latency as model objectives to find the best architecture. . Essentially, the Controller finds a model architecture, this model architecture is then used to train on ImageNet, it’s accuracy and latency values are calculated. Then, reward function is calculated and feedback is sent back to controller. We repeat this process a few times until the optimum architecture is achieved such that it’s accuracy is maximum given latency is lower than certain specified value. . The objective function can formally be defined as: . . Using the above as reward function, the authors were able to find the MNasNet architecture that achieved 75.2% top-1 accuracy and 78ms latency. More about this approach has been explained here. . Nueral Architecture Search for EfficientNets . The authors of the EfficientNet research paper used the similar approach as explained above to then find an optimal nueral network architecture that maximises ACC(m)×[FLOPS(m)/T]w. Note that for EfficientNets, the authors used FLOPS instead of latency in the objective function since the authors were not targeting specific hardware as opposed to MNasNet architecture. . From the paper: . Our search produces an efficient network, which we name EfficientNet-B0. Since we use the same search space as (Tan et al., 2019), the architecture is similar to MnasNett, except our EfficientNet-B0 is slightly bigger due to the larger FLOPS target (our FLOPS target is 400M). . The authors named this architecture as EfficientNet-B0 and it is defined in table-2 shown below again for reference: . . Since, the authors of EfficientNets used the same approach and similar nueral network search space as MNasNet, the two architectures are very similar. . So, the key question now is - what’s this MBConv layer? As I have mentioned before, it is nothing but an inverted residual bottleneck. . This has been explained further in the next section. . Inverted Bottleneck MBConv . . As in the case of Bottleneck layers that were introduced in the InceptionV2 architecture, the key idea is to first use a 1x1 convolution to bring down the number of channels and apply the 3x3 or 5x5 convolution operation to the reduced number of channels to get output features. Finally, use another 1x1 convolution operation to again increase the number of channels to the initial value. Bottleneck design used in ResNets has been shown below. . . The inverted bottleneck as in MBConv does the reverse - instead of reducing the number of channels, the first 1x1 conv layer increases the number of channels to 3 times the initial. . Note that using a standard convolution operation here would be computationally expensive, so a Depthwise Convolution is used to get the output feature map. Finally, the second 1x1 conv layer downsamples the number of channels to the initial value. This has been illustrated in fig-7. . Now you might ask what’s a Depthwise Convolution? It has been explained very well here. . So to summarize, the EfficientNet-B0 architecture uses this inverted bottleneck with Depthwise Convolution operation. But, to this, they also add squeeze and excitation operation which have been explained in my previous blog post here. . From the paper: . The main building block of EfficientNet-B0 is mobile inverted bottleneck MBConv (Sandler et al., 2018; Tan et al., 2019), to which we also add squeeze-and-excitation optimization (Hu et al., 2018). . That’s all the magic - explained. . What have we learnt so far . Before looking at the code implementations, let’s summarize in simple words what we have learnt so far. First, we looked at the idea of compound scaling depth, width and image resolution all at the same time instead of the conventional method of scaling only one of the three. Next, we also looked at the various experiments and effects of scaling each dimension on model accuracy. . We also realized that the baseline network to which compound scaling is applied also matters a lot to get best gains. The authors therefore, used Nueral Architecture Search to get a mobile-size network that’s very similar to MNasNet and they named it EfficientNet. Particularly, this baseline network is termed Efficient-B0. . Next, the authors scaled this baseline network using compound scaling to scale depth, width and resolution to get Efficient B1-B7. This process has also been summarized in the image below. . .",
            "url": "https://amaarora.github.io/fastexplain/2020/08/15/2020-08-13-efficientnet.html",
            "relUrl": "/2020/08/15/2020-08-13-efficientnet.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Groupnorm",
            "content": "Group Normalization . Group Normalization Introduction Drawback of Batch Normalization | Introduction to Group Normalization | | Other Normalization Techniques Group Normalization in detail and comparison to other normalization techniques | Group Normalization Explained | Benefits of Group Normalization over other techniques | | Number of Groups hyperparameter in Group Normalization Group Division Experiments Explained | | Effect of Group Normalization on deeper models | Implementation of GroupNorm | Does GroupNorm really work in practice? | Conclusion | References | Credits | | Introduction . In this blog post today, we will look at Group Normalization research paper and also look at: . The drawback of Batch Normalization for smaller batch sizes | Introduction to Group Normalization as an alternative to BN | Other normalization techniques available and how does Group Normalization compare to those | Benefits of Group Normalization over other normalization techniques | Discuss the optimal number of groups as a hyperparameter in GN | Discuss effect of Group Normalization on deeper models (eg. Resnet-101) | Implement Group Normalization in PyTorch and Tensorflow | Implement ResNet-50 with [GroupNorm + Weight Standardization] on Pets dataset and compare performance to vanilla ResNet-50 with BatchNorm layer | . Batch Normalization is used in most state-of-the art computer vision to stabilise training. BN normalizes the features based on the mean and variance in a mini-batch. This has helped improve model performance, reduce training time and also helped very deep models converge. . But this technique also suffers from drawbacks - if batch size is too small, training becomes unstable with BN. . The aim of this blog post is not to study BN, many other wonderful posts have been written on that, but to look at other alternatives such as GN. . Through this blog post, I hope to introduce Group Normalization as an alternative to Batch Normalization and help the reader develop an intuition for cases where GN could perform better than BN. . Drawback of Batch Normalization . Knowingly or unknowingly, we have all used BN in our experiments when training a deep learning network. If you have trained a ResNet model or pretty much any other CV model using PyTorch or Tensorflow, you have made use of BN to normalize the deep learning network. . From the Group Normalization research paper, . We all know that BN has been established as a very effective component in deep learning. BN normalizes the features by the mean and variance computed within a batch. But despite its great success, BN exhibits drawbacks that are also caused by its distinct behavior of normalizing along the batch dimension. In particular, it is required for BN to work with sufficiently large batch size. A small batch size leads to innacurate estimation of the batch statistics and reducing BN’s batch size increases the model error dramatically. . Essentially, what that means is that BN is not very effective if the batch sizes are too small. Especially for CV applications other than Image classification such as object detection, segmentation, video classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes. . Especially in such cases, GN can be used a strong alternative to BN. . Or, there could be cases where you might want to try a bigger capacity model leaving less space in the GPU to fit a bigger batch size. In such cases as well, you might want to try GN as an alternative. . Introduction to Group Normalization . In the paper, the authors introduce GN as a simple alternative to BN. From the paper: . GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. . Essentially, GN takes away the dependance on batch size for normalization and in doing so mitigates the problem suffered by BN. There are also other techniques that have been proposed to avoid batch dimension - but we will discuss them later. For now, it is essential for the reader to realize that instead of normalizing accross the batch dimension, GN normalizes accross the groups (channel dimension). This has been further explained in depth later in this post here. . First, let’s look at how GN compares to BN for training accross various batch sizes keeping all else same. . . As can be seen in the image above, because GN does not depend on the batch size, the validation classification error (when the deep learning model is normalized using GN) is stable accross various batch sizes compared to BN. . . The same trend as in fig-1 can also be observed in fig-2 where the validation error is consistent accross various batch sizes for GN as opposed to BN. Another key thing to note, the validation error for GN as reported in the research paper is very similar to that for BN - therefore, GN can be considered to be a strong alternative to BN. . The validation errors (from the research paper) for various batch sizes are presented in table-1 below: . . While BN performs slightly better than GN for batch size 32, GN performs better for all lower batch sizes. . Other Normalization Techniques . Group Normalization isn’t the first technique that was proposed to overcome the drawback of BN. There are also several other techniques such as Layer Normalization, Instance Normalization and others mentioned in the references of this blog post. . But, GN is the first technique to achieve comparable validation error rates as compared to BN. . In this section we look at the most popular normalization tecniques namely - Layer Normalization (LN), Instance Normalization (IN), Batch Normalization (BN) and Group Normalization (GN). . Group Normalization in detail and comparison to other normalization techniques . . The above image presented in the research paper is one of the best ways to compare the various normalization techniques and get an intuitive understanding for GN. . Let’s consider that we have a batch of dimension (N, C, H, W) that needs to be normalized. . Here, . N: Batch Size | C: Number of Channels | H: Height of the feature map | W: Width of the feature map | . Essentially, in BN, the pixels sharing the same channel index are normalized together. That is, for each channel, BN computes the mean and std deviation along the (N, H, W) axes. As we can see, the group statistics depend on N, the batch size. . In LN, the mean and std deviation are computed for each sample along the (C, H, W) axes. Therefore, the calculations are independent of the batch size. . In IN, the mean and std deviation are computed for each sample and each channel along the (H, W) axes. Again, the calculations are independent of batch size. . Group Normalization Explained . . Finally, for group norm, the batch is first divided into groups (32 by default, discussed later). The batch with dimension (N, C, W, H) is first reshaped to (N, G, C//G, H, W) dimensions where G represents the number of groups. Finally, the mean and std deviation are calculated along the groups, that is (H, W) and along C//G channels. This is also illustrated very well in fig-4. . One key thing to note here, if C == G, that is the number of groups are set to be equal to the number of channels (one channel per group), then GN becomes IN. . And if, G == 1, that is number of groups is set to 1, GN becomes LN. . I would like for the reader to take a minute here and make sure that he/she understands the differences between these normalization techniques mentioned above. . Benefits of Group Normalization over other techniques . Also, it is important to note that GN is less restricted than LN, because in LN it is assumed that all channels in a layer make “equal contributions” whereas GN is more flexible because in GN, each group of channels (instead of all of them) are assumed to have shared mean and variance - the model still has flexibility of learning a different distribution for each group. . Also, GN is slightly better than IN because IN normalizes accross each sample for each channel, therefore, unlike GN, it misses the opportunity of exploiting the channel dependence. . . Therefore, due to the reasons discussed above, we can see that the validation and training errors for GN are lower than those for LN and IN. . Number of Groups hyperparameter in Group Normalization . One key hyperparameter in Group Normalization is the number of groups to divide the channels into. . . The authors of the research paper ran an experiment to train ResNet-50 model on Imagenet dataset using various number of groups. . As can be seen in table-2, setting number of groups to 32 achieves the lowest validation error. . In the bottom part of table-2, the authors set a fixed number of channels per group. Essentially, since each layer in a deep learning model can have various number of channels, this means there are varying number of groups per layer. Setting 16 channels per group achieved the lowest score. . Group Division Experiments Explained . . Let’s understand what’s going on with help of VGGNet. As can be seen, there are varying number of channels in different layers of VGGNet (this is also the case for other deep learning models like ResNet, DenseNet etc). The authors essentially in the first experiment, divide each layer into 32 groups. Thus for layer 2 of VGGNet with 128 #channels, there are 128//32, that is, 4 channels per group if group number is set to 32. The authors ran this experiments for varying number of groups and found for number of groups set to 32 to have the lowest validtion error. . For the second experiment, the authors set the number of channels per group fixed. For example, if number of channels per group was set to 16, then the second layer with 128 channels had 128//16, that is, 8 groups and the third layer with 256 channels had 256//16, 16 groups and so on. The authors found setting 16 channels per group to have to have the lowest validation error. . Effect of Group Normalization on deeper models . The authors also ran experiments and trained ResNet-101 architecture for batch size 32 and compared the validation errors with BN and GN implementation. The authors found the BN baseline to have 22.0% validation error and the GN counterpart to have 22.4% validation error. Also, for batch size 2, the authors found the GN error to be 23.0% which is still a very decent result considering the very small batch size. . Thus, I think from the results of this experiment, it is safe to say that GN with smaller batch sizes also works for larger models. . Implementation of GroupNorm . Finally, we are now ready to look at the implementation of GN. . The following snippet of code has been provided in the research paper: . def GroupNorm(x, gamma, beta, G, eps=1e−5): # x: input features with shape [N,C,H,W] # gamma, beta: scale and offset, with shape [1,C,1,1] # G: number of groups for GN N, C, H, W = x.shape x = tf.reshape(x, [N, G, C // G, H, W]) mean, var = tf.nn.moments(x, [2, 3, 4], keep dims=True) x = (x − mean) / tf.sqrt(var + eps) x = tf.reshape(x, [N, C, H, W]) return x ∗ gamma + beta . Essentially, the authors reshape the batch and divide into groups with C // G channels per group where, . C: number of channels | G: number of groups | . Finally, as discussed in this section, the authors normalize along the (C//G, H, W) dimension and return the result after reshaping the batch back to (N, C, H, W). . I hope that by this time, the implementation should be clear to the reader. If it isn’t, either I have not explained GN very well, or I kindly ask the reader to go back to Group Normalization Explained section and have a quick re-read. . Finally, we could rewrite GN in PyTorch like so: . import torch import torch.nn as nn class GroupNorm(nn.Module): def __init__(self, num_features, num_groups=32, eps=1e-5): super(GroupNorm, self).__init__() self.gamma = nn.Parameter(torch.ones(1,num_features,1,1)) self.beta = nn.Parameter(torch.zeros(1,num_features,1,1)) self.num_groups = num_groups self.eps = eps def forward(self, x): N, C, H, W = x.size() x = x.view(N, self.num_groups ,-1) mean = x.mean(-1, keepdim=True) var = x.var(-1, keepdim=True) # normalize x = (x-mean) / (var+self.eps).sqrt() x = x.view(N,C,H,W) return x * self.gamma + self.beta . PyTorch also inherently supports GroupNorm and can be used by using nn.GroupNorm. . Having implemented GN in PyTorch and Tensorflow, we are now ready to run our own experiments and see the results for ourselves in the next section. . Does GroupNorm really work in practice? . Personally, I wanted to try a little experiment of my own to compare GN with BN and corroborate the findings in the GN research paper. . You can find the experiment in this notebook here. . Basically, in the experiment, I trained two ResNet-34 architectures on the Pets dataset - one with BN and other with GN. To my surprise, I found that simply replacing BatchNorm with GroupNorm led to sub-optimal results and the model with GroupNorm used as the normalization layer performed much worse than the model normalized with BatchNorm layer even for a very small batch size of 4. This was very different to the results reported in fig-1. . Thanks to Sunil Kumar who pointed me to Big Transfer (BiT): General Visual Representation Learning research paper where I noticed that the researchers used a combination of Weight Standardization and GN to achieve SOTA results. So I tried this out with the implementation of Weight Standardization as in the official repository here and very quickly I was able to replicate the results with GN + WS performing significantly better than BN for batch size of 1 here. . Conclusion . I hope that I have been clear in my explaination of Group Normalization, and also through my experiments, I have been able to provide a way for you to implement GN in PyTorch and Tensorflow and run experiments of your own. . As always, constructive feedback is always welcome at @amaarora. . Also, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading! . References . Group Normalization by He et al | Batch Normalization by Ioffe et al | Instance Normalization: The Missing Ingredient for Fast Stylization | Layer Normalization | Weight Standardization | Implementation of Weight Standardization from the official repository | Deep Residual Learning for Image Recognition | Credits . Thanks to @AryMob for pointing out errata in this post. .",
            "url": "https://amaarora.github.io/fastexplain/2020/08/09/groupnorm.html",
            "relUrl": "/2020/08/09/groupnorm.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Densenets",
            "content": "DenseNet Architecture Explained with PyTorch Implementation from TorchVision . DenseNet Architecture Explained with PyTorch Implementation from TorchVision Introduction | DenseNet Architecture Introduction | But is feature concatenation possible? Transition Layers | | Dense connectivity Inside a single DenseBlock | | DenseNet Architecture as a collection of DenseBlocks Bottleneck Layers | | DenseNet Implementation DenseLayer Implementation | DenseBlock Implementation | | DenseNet Architecture Implementation | Conclusion | Credits | | Introduction . In this post today, we will be looking at DenseNet architecture from the research paper Densely Connected Convolutional Networks. . The overall agenda is to: . Understand what DenseNet architecture is | Introduce dense blocks, transition layers and look at a single dense block in more detail | Understand step-by-step the TorchVision implementation of DenseNet | . DenseNet Architecture Introduction . In a standard Convolutional Neural Network, we have an input image, that is then passed through the network to get an output predicted label in a way where the forward pass is pretty straightforward as shown in the image below: . . Each convolutional layer except the first one (which takes in the input image), takes in the output of the previous convolutional layer and produces an output feature map that is then passed to next convolutional layer. For L layers, there are L direct connections - one between each layer and its subsequent layer. . The DenseNet architecture is all about modifying this standard CNN architecture like so: . . In a DenseNet architecture, each layer is connected to every other layer, hence the name Densely Connected Convolutional Network. For L layers, there are L(L+1)/2 direct connections. For each layer, the feature maps of all the preceding layers are used as inputs, and its own feature maps are used as input for each subsequent layers. . This is really it, as simple as this may sound, DenseNets essentially conect every layer to every other layer. This is the main idea that is extremely powerful. The input of a layer inside DenseNet is the concatenation of feature maps from previous layers. . From the paper: . DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. . But is feature concatenation possible? . Okay, so then, now we know the input of Lth layer are the feature maps from [L1, L1, L1.. L-1th] concatenated but is this concatenation possible? . At this point in time, I want you to think about whether we can concat the features from the first layer of a DenseNet with the last layer of the DenseNet? If we can, why? If we can’t, what do we need to do to make this possible? . This is a good time to take a minute and think about this question. . So, here’s what I think - it would not be possible to concatenate the feature maps if the size of feature maps is different. So, to be able to perform the concatenation operation, we need to make sure that the size of the feature maps that we are concatenating is the same. Right? . But we can’t just keep the feature maps the same size throughout the network - an essential part of concvolutional networks is down-sampling layers that change the size of feature maps. For example, look at the VGG architecture below: . . The input of shape 224x224x3 is downsampled to 7x7x512 towards the end of the network. . To facilitate both down-sampling in the architecture and feature concatenation - the authors divided the network into multiple densely connected dense blocks. Inside the dense blocks, the feature map size remains the same. . . Dividing the network into densely connected blocks solves the problem that we discussed above. . Now, the Convolution + Pooling operations outside the dense blocks can perform the downsampling operation and inside the dense block we can make sure that the size of the feature maps is the same to be able to perform feature concatenation. . Transition Layers . The authors refer to the layers between the dense blocks as transition layers which do the convolution and pooling. . From the paper, we know that the transition layers used in the DenseNet architecture consist of a batch-norm layer, 1x1 convolution followed by a 2x2 average pooling layer. . Given that the transition layers are pretty easy, let’s quickly implement them here: . class _Transition(nn.Sequential): def __init__(self, num_input_features, num_output_features): super(_Transition, self).__init__() self.add_module(&#39;norm&#39;, nn.BatchNorm2d(num_input_features)) self.add_module(&#39;relu&#39;, nn.ReLU(inplace=True)) self.add_module(&#39;conv&#39;, nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)) self.add_module(&#39;pool&#39;, nn.AvgPool2d(kernel_size=2, stride=2)) . Essentially the 1x1 conv performs the downsampling from num_input_features to num_output_features. . Dense connectivity . Let’s consider a network with L layers, each of which performs a non-linear transformation HL. The output of the Lth layer of the network is denoted as xL and the input image is represented as x0. . We know that traditional feed-forward netowrks connect the output of the Lth layer to L+1th layer. And the skip connection can be represented as: . . In DenseNet architecture, the dense connectivity can be represented as: . . where [x0, x1, x2..] represents concatenation of the feature maps produced by [0,1,.. Lth] layers. . Inside a single DenseBlock . Now that we understand that a DenseNet architecture is divided into multiple dense blocks, let’s look at a single dense block in a little more detail. Essentially, we know, that inside a dense block, each layer is connected to every other layer and the feature map size remains the same. . . Let’s try and understand what’s really going on inside a dense block. We have some gray input features that are then passed to LAYER_0. The LAYER_0 performs a non-linear transformation to add purple features to the gray features. These are then used as input to LAYER_1 which performs a non-linear transformation to also add orange features to the gray and purple ones. And so on until the final output for this 3 layer denseblock is a concatenation of gray, purple, orange and green features. . So, in a dense block, each layer adds some features on top of the existing feature maps. . Therefore, as you can see the size of the feature map grows after a pass through each dense layer and the new features are concatenated to the existing features. One can think of the features as a global state of the network and each layer adds K features on top to the global state. . This parameter K is referred to as growth rate of the network. . DenseNet Architecture as a collection of DenseBlocks . We already know by now from fig-4, that DenseNets are divided into multiple DenseBlocks. . The various architectures of DenseNets have been summarized in the paper. . . Each architecture consists of four DenseBlocks with varying number of layers. For example, the DenseNet-121 has [6,12,24,16] layers in the four dense blocks whereas DenseNet-169 has [6, 12, 32, 32] layers. . We can see that the first part of the DenseNet architecture consists of a 7x7 stride 2 Conv Layer followed by a 3x3 stride-2 MaxPooling layer. And the fourth dense block is followed by a Classification Layer that accepts the feature maps of all layers of the network to perform the classification. . Also, the convolution operations inside each of the architectures are the Bottle Neck layers. What this means is that the 1x1 conv reduces the number of channels in the input and 3x3 conv performs the convolution operation on the transformed version of the input with reduced number of channels rather than the input. . Bottleneck Layers . By now, we know that each layer produces K feature maps which are then concatenated to previous feature maps. Therefore, the number of inputs are quite high especially for later layers in the network. . This has huge computational requirements and to make it more efficient, the authors decided to utilize Bottleneck layers. From the paper: . 1×1 convolution can be introduced as bottleneck layer before each 3×3 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. In our experiments, we let each 1×1 convolution produce 4k feature-maps. . We know K refers to the growth rate, so what the authors have finalized on is for 1x1 conv to first produce 4*K feature maps and then perform 3x3 conv on these 4*k size feature maps. . DenseNet Implementation . We are now ready and have all the building blocks to implement DenseNet in PyTorch. . DenseLayer Implementation . The first thing we need is to implement the dense layer inside a dense block. . class _DenseLayer(nn.Module): def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False): super(_DenseLayer, self).__init__() self.add_module(&#39;norm1&#39;, nn.BatchNorm2d(num_input_features)), self.add_module(&#39;relu1&#39;, nn.ReLU(inplace=True)), self.add_module(&#39;conv1&#39;, nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)), self.add_module(&#39;norm2&#39;, nn.BatchNorm2d(bn_size * growth_rate)), self.add_module(&#39;relu2&#39;, nn.ReLU(inplace=True)), self.add_module(&#39;conv2&#39;, nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)), self.drop_rate = float(drop_rate) self.memory_efficient = memory_efficient def bn_function(self, inputs): &quot;Bottleneck function&quot; # type: (List[Tensor]) -&gt; Tensor concated_features = torch.cat(inputs, 1) bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features))) # noqa: T484 return bottleneck_output def forward(self, input): # noqa: F811 if isinstance(input, Tensor): prev_features = [input] else: prev_features = input bottleneck_output = self.bn_function(prev_features) new_features = self.conv2(self.relu2(self.norm2(bottleneck_output))) if self.drop_rate &gt; 0: new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) return new_features . A DenseLayer accepts an input, concatenates the input together and performs bn_function on these feature maps to get bottleneck_output. This is done for computational efficiency. Finally, the convolution operation is performed to get new_features which are of size K or growth_rate. . It should now be easy to map the above implementation with fig-5 shown below for reference again: . . Let’s say the above is an implementation of LAYER_2. First, LAYER_2 accepts the gray, purple, orange feature maps and concatenates them. Next, the LAYER_2 performs a bottleneck operation to create bottleneck_output for computational efficiency. Finally, the layer performs the HL operation as in eq-2 to generate new_features. These new_features are the green features as in fig-5. . Great! So far we have successfully implemented Transition and Dense layers. . DenseBlock Implementation . Now, we are ready to implement the DenseBlock which consists of multiple such DenseLayers. . class _DenseBlock(nn.ModuleDict): _version = 2 def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False): super(_DenseBlock, self).__init__() for i in range(num_layers): layer = _DenseLayer( num_input_features + i * growth_rate, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate, memory_efficient=memory_efficient, ) self.add_module(&#39;denselayer%d&#39; % (i + 1), layer) def forward(self, init_features): features = [init_features] for name, layer in self.items(): new_features = layer(features) features.append(new_features) return torch.cat(features, 1) . Let’s map the implementation of this DenseBlock with fig-5 again. Let’s say we pass the number of layers num_layers as 3 to create fig-5 block. In this case, let’s imagine that the num_input_features in gray in the figure is 64. We already know that the authors choose the bottleneck size bn_size for 1x1 conv to be 4. Let’s consider the growth_rate is 32 (same for all networks as in the paper). . Great, so the first layer LAYER_0 accepts 64 num_input_features and outputs extra 32 features. Excellent. Now, LAYER_1 accepts the 96 features num_input_features + 1 * growth rate and outputs extra 32 features again. Finally, LAYER_2 accepts 128 features num_input_features + 2 * growth rate and adds the 32 green features on top with are then concatenated to existing features and returned by the DenseBlock. . At this stage, it should be really easy for you to map the implementation of dense block with fig-5. . DenseNet Architecture Implementation . Finally, we are now ready to implement the DenseNet architecture as we have already implemented the DenseLayer and DenseBlock. . class DenseNet(nn.Module): def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False): super(DenseNet, self).__init__() # Convolution and pooling part from table-1 self.features = nn.Sequential(OrderedDict([ (&#39;conv0&#39;, nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), (&#39;norm0&#39;, nn.BatchNorm2d(num_init_features)), (&#39;relu0&#39;, nn.ReLU(inplace=True)), (&#39;pool0&#39;, nn.MaxPool2d(kernel_size=3, stride=2, padding=1)), ])) # Add multiple denseblocks based on config # for densenet-121 config: [6,12,24,16] num_features = num_init_features for i, num_layers in enumerate(block_config): block = _DenseBlock( num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, memory_efficient=memory_efficient ) self.features.add_module(&#39;denseblock%d&#39; % (i + 1), block) num_features = num_features + num_layers * growth_rate if i != len(block_config) - 1: # add transition layer between denseblocks to # downsample trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2) self.features.add_module(&#39;transition%d&#39; % (i + 1), trans) num_features = num_features // 2 # Final batch norm self.features.add_module(&#39;norm5&#39;, nn.BatchNorm2d(num_features)) # Linear layer self.classifier = nn.Linear(num_features, num_classes) # Official init from torch repo. for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.constant_(m.bias, 0) def forward(self, x): features = self.features(x) out = F.relu(features, inplace=True) out = F.adaptive_avg_pool2d(out, (1, 1)) out = torch.flatten(out, 1) out = self.classifier(out) return out . Let’s use the above implementation to create densenet-121 architecture. . def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress, **kwargs): model = DenseNet(growth_rate, block_config, num_init_features, **kwargs) return model def densenet121(pretrained=False, progress=True, **kwargs): return _densenet(&#39;densenet121&#39;, 32, (6, 12, 24, 16), 64, pretrained, progress, **kwargs) . Here’s what happens. First, we initialize the stem of the DenseNet architecture - this is the convolution and pooling part from table-1. . This part of the code does that: . self.features = nn.Sequential(OrderedDict([ (&#39;conv0&#39;, nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), (&#39;norm0&#39;, nn.BatchNorm2d(num_init_features)), (&#39;relu0&#39;, nn.ReLU(inplace=True)), (&#39;pool0&#39;, nn.MaxPool2d(kernel_size=3, stride=2, padding=1)), ])) . Next, based on the config, we create a DenseBlock based on the number of layers in the config. . This part of the code does this: . for i, num_layers in enumerate(block_config): block = _DenseBlock( num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, memory_efficient=memory_efficient ) self.features.add_module(&#39;denseblock%d&#39; % (i + 1), block) . Finally, we add Transition Layers between DenseBlocks. . if i != len(block_config) - 1: # add transition layer between denseblocks to # downsample trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2) self.features.add_module(&#39;transition%d&#39; % (i + 1), trans) num_features = num_features // 2 . And that’s all the magic behind DenseNets! . Conclusion . Congratulations! Today, together, we successfully understood what DenseNets are and also understood the torchvision implementation of DenseNets. I hope that by now you have a very thorough understanding of the DenseNet architecture. . As always, constructive feedback is always welcome at @amaarora. . Also, feel free to subscribe to my blog here to receive regular updates regarding new blog posts. Thanks for reading! . Credits . All code implementations have been directly copied from torchvision. .",
            "url": "https://amaarora.github.io/fastexplain/2020/08/02/densenets.html",
            "relUrl": "/2020/08/02/densenets.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Senet",
            "content": "Squeeze and Excitation Networks Explained with PyTorch Implementation . Squeeze and Excitation Networks Explained with PyTorch Implementation Introduction | Intuition behind Squeeze-and-Excitation Networks Main Idea behind Se-Nets: | | Squeeze: Global Information Embedding | Excitation: Adaptive Recalibration | Squeeze and Excitation Block in PyTorch | SE Block with Existing SOTA Architectures | SE-ResNet in PyTorch SEResNet-18 | SEResNet-34 | SEResNet-50 | SEResNet-101 | | Conclusion | Credits | | Introduction . In this blog post, we will be looking at the Squeeze-and-Excitation networks. We will refer to the research paper by Hu et al and first understand what Squeeze-and-Excitation networks are before implementing the novel architecture in PyTorch with very few modifications to the popular ResNet architecture. . First, we develop an intuition for what SE-Nets are and the novel idea behind their success. Next, we will look at the Squeeze and Excitation operations in a little more detail. Finally, we implement the Squeeze-and-Excitation networks in PyTorch with very minor updates to ResNet implementation in torchvision. . Intuition behind Squeeze-and-Excitation Networks . So, what’s new in the Squeeze-and-Excitation networks? How are they different from the ResNet architecture? . Let’s consider an RGB image as an input. Then the convolution operation with a 3x3 kernel on the input image can be visualised as below: . . A feature map is generated per-channel (RGB) and then summed together to form one channel or the final output of the convolution operation as below: . . Implicitly, the convolution kernel would have different weights for different channels and these are learned weights through backpropagation. If it’s an RGB image, then generally the kernels are also cubic to map channel dependencies. . Main Idea behind Se-Nets: . We expect the learning of convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able to increase its sensitivity to informative features which can be exploited by subsequent transformations. Consequently, we would like to provide it with access to global information and recalibrate filter responses in two steps, squeeze and excitation, before they are fed into the next transformation. . In other words, with the squeeze-and-excitation block, the nueral nets are better able to map the channel dependency along with access to global information. Therefore, they are better able to recalibrate the filter outputs and thus, this leads to performance gains. . . This main idea can be further explained using the Squeeze-and-Excitation block image above from the paper. First, a feature transformation (such as a convolution operation) is performed on the input image X to get features U. Next, we perform a squeeze operation to get a single value for each channel of output U. After, we perform an excitation operation on the output of the squeeze operation to get per-channel weights. . Finally, once we have the per-channel weights, the final output of the block is obtained by rescaling the feature map U with these activations. . From the paper: . The role this operation performs at different depths differs throughout the network. In earlier layers, it excites informative features in a class-agnostic manner, strengthening the shared low-level representations. In later layers, the SE blocks become increasingly specialised, and respond to different inputs in a highly class-specific manner. As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network. . Next, we will look at the Squeeze and Excitation operations in a little more detail. . Squeeze: Global Information Embedding . The main purpose of the Squeeze operation is to extract global information from each of the channels of an image. Since, convolution is a local operation (that is, at a particular time, it is only able to see a part of the image), it might be beneficial to also extract information outside the receptive field of the convolution filter. . The Squeeze operation is meant to do exactly that and the authors keep it as simple as possible. . The authors perform a Global Average Pooling operation to reduce the C x H x W image to C x 1 x 1 to get a global statistic for each channel. . Formally, the Global Average Pooling or the Squeeze operation can be formally represented as: . . In other words, all we do is that we take the mean of each channel across H x W spatial dimensions. . Excitation: Adaptive Recalibration . Now that we have a vector of length C from the Squeeze operation, the next step is to generate a set of weights for each channel. This is done with the help of Excitation operation explained in this section. . Formally, the excitation operation can be represented by: . . where: . δ refers to ReLU operation | σ refers to Sigmoid operation | W1 and W2 are two fully-connected layers | z is the output from the Squeeze block | . The two FC layers form a bottleneck architecture, that is, the first W1 layer is used for dimensionality reduction by a ratio r and the second W2 layer is a dimensionality increasing layer returning to the channel dimension of U. . Since, the Sigmoid layer would return numbers between 0 and 1, these are the channel weights and the final output of the block can be respresented as: . . From the paper: . The excitation operator maps the input-specific descriptor z to a set of channel weights. In this regard, SE blocks intrinsically introduce dynamics conditioned on the input, which can be regarded as a self-attention function on channels whose relationships are not confined to the local receptive field the convolutional filter are responsive to. . The authors in the paper also mentioned the thinking and reasoning behind coming up with this excitation function. They write: . To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfil this objective, the function must meet two criteria: first, it must be flexible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised. . That is why using a Sigmoid layer makes so much sense rather than Softmax(which would generally impose importance on only one of the channels). A Sigmoid function (which is also used in multi-label classification) allows multiple channels to have higher importance. . Squeeze and Excitation Block in PyTorch . class SE_Block(nn.Module): &quot;credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4&quot; def __init__(self, c, r=16): super().__init__() self.squeeze = nn.AdaptiveAvgPool2d(1) self.excitation = nn.Sequential( nn.Linear(c, c // r, bias=False), nn.ReLU(inplace=True), nn.Linear(c // r, c, bias=False), nn.Sigmoid() ) def forward(self, x): bs, c, _, _ = x.shape y = self.squeeze(x).view(bs, c) y = self.excitation(y).view(bs, c, 1, 1) return x * y.expand_as(x) . As mentioned the Squeeze operation is a global Average Pooling operation and in PyTorch this can be represented as nn.AdaptiveAvgPool2d(1) where 1, represents the output size. . Next, the Excitation network is a bottle neck architecture with two FC layers, first to reduce the dimensions and second to increase the dimensions back to original. We reduce the dimensions by a reduction ratio r=16. This is as simple as creating a nn.Sequential with two FC layers, with a nn.ReLU() in between and followed by a nn.Sigmoid(). . The outputs of the Excitation operation are the channel weights which are then multiplied element-wise to input feature X to get the final output of the SE_Block. . SE Block with Existing SOTA Architectures . From the paper: . The structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced. SE blocks are also computationally lightweight and impose only a slight increase in model complexity and computational burden. . In other words, it is really simple to integrate SE blocks with existing state-of-art architectures. The authors provide two examples for SE-ResNet and SE-Inception as below: . . In this blogpost we will implement the SE-ResNet architecture and the SE-Inception architecture is left as an exercise to the reader. . The authors experimented by inserting SE block in various positions and found that the performance improvements produced by SE units are fairly robust to their location, provided that they are applied prior to branch aggregation. In this post, we will construct a SE-ResNet architecture using the Standard SE block integration as below: . . Key point to note: . As can be seen from the Standard SE block (b) integration, the SE block is preceded directly by the Residual operation in ResNet before summation with the identity branch. . SE-ResNet in PyTorch . As we saw from fig-6, we simply insert the SE block after the Residual operation. To create SE-Resnet34 and below, we simply copy the BasicBlock from torchvision from here. . class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;) if dilation &gt; 1: raise NotImplementedError(&quot;Dilation &gt; 1 not supported in BasicBlock&quot;) # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out . Next, we update the forward to insert the SE block operation as in fig-6: . class SEBasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None, r=16): super(SEBasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;) if dilation &gt; 1: raise NotImplementedError(&quot;Dilation &gt; 1 not supported in BasicBlock&quot;) # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride # add SE block self.se = SE_Block(planes, r) def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) # add SE operation out = self.se(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out . And that’s it! As easy as it may sound, we have just implemented the SEBasicBlock in PyTorch. . Next, for ResNet-50 and above, we perform the same steps for Bottleneck architecture. First, we copy the Bottleneck class from torchvision. . class Bottleneck(nn.Module): # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) # while original implementation places the stride at the first 1x1 convolution(self.conv1) # according to &quot;Deep residual learning for image recognition&quot;https://arxiv.org/abs/1512.03385. # This variant is also known as ResNet V1.5 and improves accuracy according to # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out . Next, we add the SE_Block operation similar to what we did for the BasicBlock as below: . class SEBottleneck(nn.Module): # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) # while original implementation places the stride at the first 1x1 convolution(self.conv1) # according to &quot;Deep residual learning for image recognition&quot;https://arxiv.org/abs/1512.03385. # This variant is also known as ResNet V1.5 and improves accuracy according to # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None, r=16): super(SEBottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride # Add SE block self.se = SE_Block(planes, r) def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) # Add SE operation out = self.se(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out . That’s it! Now that we have implemented SEBasicBlock and SEBottleneck in PyTorch, we are ready to construct SE-ResNet architectures. As was mentioned in the paper, . The structure of the SE block is simple and can be used directly in existing state-of-the-art architectures by replacing components with their SE counterparts, where the performance can be effectively enhanced. . Let’s do exactly this. We simply replace the components of ResNet architecture with the SE counterparts. . But, one last step is to copy some helper functions from torchvision. These functions are present here: . def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1): &quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation) def conv1x1(in_planes, out_planes, stride=1): &quot;&quot;&quot;1x1 convolution&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) def _resnet(arch, block, layers, pretrained, progress, **kwargs): model = ResNet(block, layers, **kwargs) return model . SEResNet-18 . From torchvision here, we update the implementation of resnet18 to get se_resnet18: . def se_resnet18(pretrained=False, progress=True, **kwargs): return _resnet(&#39;resnet18&#39;, SEBasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs) . SEResNet-34 . From torchvision here, we update the implementation of resnet34 to get se_resnet34: . def se_resnet34(pretrained=False, progress=True, **kwargs): return _resnet(&#39;resnet34&#39;, SEBasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs) . SEResNet-50 . From torchvision here, we update the implementation of resnet50 to get se_resnet50: . def se_resnet50(pretrained=False, progress=True, **kwargs): return _resnet(&#39;resnet50&#39;, SEBottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) . SEResNet-101 . From torchvision here, we update the implementation of resnet101 to get se_resnet101: . def se_resnet101(pretrained=False, progress=True, **kwargs): return _resnet(&#39;resnet101&#39;, SEBottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) . Conclusion . In this blogpost, first, we looked at what SE blocks are and the novel idea that they introduce. Next, we looked at the Squeeze and Excitation operation that is used to generate per-channel weights which are then used to return the final output of the SE block. . Finally, we looked at integration of SE block in the ResNet architecture to construct se_resnet18, se_resnet34, se_resnet50 and se_resnet101. . I hope that my explaination of SE Blocks was clear and as always - constructive feedback is always welcome at @amaarora. . Also, feel free to subscribe to receive regular updates regarding new blog posts. Thanks for reading! . Credits . The implementation of SE_Block has been adapted from senet.pytorch repo here. . Also, thanks to my friends - Atmadeep Banerjee and Akash Palrecha - both research interns at Harvard - for proof reading the draft version of this blog post and providing me with an honest and constructive feedback. . Both Akash and Atmadeep found a technical error in my blog in the main idea behind SENet section where I had skipped the part that channel-dependencies are implicitly present when performing a convolution operation. Also, they helped me make this section better by helping me further improve my understanding of SeNets. . Akash, was also very kind to point out a grammatical error and also it was his suggestion to add the two reasons mentioned in the paper for choosing the excitation function. . Both Akash and Atmadeep helped me in making this blog post better and more accurate. Thanks guys! .",
            "url": "https://amaarora.github.io/fastexplain/2020/07/24/SeNet.html",
            "relUrl": "/2020/07/24/SeNet.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Label Smoothing",
            "content": "Label Smoothing Explained using Microsoft Excel . Label Smoothing Explained using Microsoft Excel Introduction | Why do we need Label Smoothing? | What is Label Smoothing? | Label Smoothing in Microsoft Excel | Fastai/PyTorch Implementation of Label Smoothing Cross Entropy loss | Comparing Microsoft Excel results with PyTorch | Conclusion | References | Credits | | Introduction . In this blogpost, together, we: . Read and understand about Label Smoothing from Rethinking the Inception Architecture for Computer Vision research paper | Look at why we need Label Smoothing? | Re-implement Label Smoothing in Microsoft Excel step by step | Compare the results from our MS Excel implementation with Fastai/PyTorch versions of Label Smoothing | . Why are we using Microsoft Excel? . It’s a valid question you might ask and I wasn’t a big fan of MS Excel either until I saw this video by Jeremy Howard about Cross Entropy Loss. In the video Jeremy explains Cross Entropy Loss using Microsoft Excel. It clicked and I understood it very well even with the fancy math in the cross entropy loss formula. . . And that is my hope here too! In this blogpost I hope that together we can see past the math and get the intuition for Label Smoothing and then later be able to implement it in a language/framework of our choice. . So, let’s get started! . Why do we need Label Smoothing? . Let’s consider we are faced with a multi-class image classification problem. Someone presents to us five images with labels - . Image Name Label . img-1.jpg | Dog | . img-2.jpg | Cat | . img-3.jpg | Horse | . img-4.jpg | Bear | . img-5.jpg | Kangaroo | . As humans, we will quickly be able to assign labels to the image just by looking at them, for example we know that img-1.jpg is that of a dog, img-2.jpg is a cat and so on. . Let’s one-hot encode the labels, so our labels get updated to: . Image Name is_dog is_cat is_horse is_bear is_kroo . img-1.jpg | 1 | 0 | 0 | 0 | 0 | . img-2.jpg | 0 | 1 | 0 | 0 | 0 | . img-3.jpg | 0 | 0 | 1 | 0 | 0 | . img-4.jpg | 0 | 0 | 0 | 1 | 0 | . img-5.jpg | 0 | 0 | 0 | 0 | 1 | . Let’s imagine that we used the above set of 5 images and the labels and trained a deep learning model which in it’s early stages learns to predict a set of logits for each class like so: . Image Name is_dog is_cat is_horse is_bear is_kroo . img-1.jpg | 4.7 | -2.5 | 0.6 | 1.2 | 0.4 | . img-2.jpg | -1.2 | 2.4 | 2.6 | -0.6 | 2.34 | . img-3.jpg | -2.4 | 1.2 | 1.1 | 0.8 | 1.2 | . img-4.jpg | 1.2 | 0.2 | 0.8 | 1.9 | -0.6 | . img-5.jpg | -0.9 | -0.1 | -0.2 | -0.5 | 1.6 | . This is pretty standard - right? This is what we do when we’re training an image classifier anyway. We pass a list of images and labels, make the model predict something, then calculate the cross-entropy loss and backpropogate to update the model’s parameters. And we keep doing this until the model learns to assign the correct labels to the corresponding images. So what’s the problem? . Here’s the important part: . For the cross-Entropy loss to really be at a minimum, each logit corresponding to the correct class needs to be significantly higher than the rest. That is, for example for row-1, img-1.jpg the logit of 4.7 corresponding to is_dog needs to be significantly higher than the rest. This is also the case for all the other rows. . A mathematical proof is presented here by Lei Mao where he explains why minimizing cross entropy loss is equivalent to do maximum likelihood estimation. . This case where, in order to minimise the cross-entropy loss, the logits corresponding to the true label need to be significantly higher than the rest can actually cause two problems. . From the paper, . This, however, can cause two problems. First, it may result in over-fitting: if the model learns to assign full probability to the ground- truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient ∂ℓ/∂z,k , reduces the ability of the model to adapt. . In other words, our model could become overconfident of it’s predictions because to really minimise the loss, our model needs to be very sure of everything that it predicts. This is bad because it is then harder for the model to generalise and easier for it to overfit to the training data. We want the model to generalize and be able to look at other dogs, cats.. images that weren’t part of the training set and still be able to predict them well. . What is Label Smoothing? . Label Smoothing was first introduced in Rethinking the Inception Architecture for Computer Vision. . From Section-7 - Model Regularization via Label Smoothing in the paper, . We propose a mechanism for encouraging the model to be less confident. While this may not be desired if the goal is to maximize the log-likelihood of training labels, it does regularize the model and makes it more adaptable. The method is very simple. Consider a distribution over labels u(k), independent of the training example x, and a smoothing parameter Є. For a training example with ground-truth label y, we replace the label distribution q(k/x) = δ(k,y) with . . which is a mixture of the original ground-truth distribution q(k|x) and the fixed distribution u(k), with weights 1 − Є. and Є, respectively. In our experiments, we used the uniform distribution u(k) = 1/K, so that . . In other words, instead of using the hard labels or the one-hot encoded variables where the true label is 1, let’s replace them with (1-Є) * 1 where Є refers to the smoothing parameter. Once that’s done, we add some uniform noise 1/K to the labels where K: total number of labels. . So the updated distribution for the our examples with label smoothing factor Є = 0.1 becomes: . Image Name is_dog is_cat is_horse is_bear is_kroo . img-1.jpg | 0.92 | 0.02 | 0.02 | 0.02 | 0.02 | . img-2.jpg | 0.02 | 0.92 | 0.02 | 0.02 | 0.02 | . img-3.jpg | 0.02 | 0.02 | 0.92 | 0.02 | 0.02 | . img-4.jpg | 0.02 | 0.02 | 0.02 | 0.92 | 0.02 | . img-5.jpg | 0.02 | 0.02 | 0.02 | 0.02 | 0.92 | . We get the updated distribution above because 1-Є = 0.9. So as a first step, we replace all the true labels with 0.9 instead of 1. Next, we add a uniform noise 1/K = 0.02 because in our case K equals 5. Finally we get the above update distribution with uniform noise. . The authors refer to the above change as label-smoothing regularization or LSR. And then we calculate the cross-entropy loss with the updated distribution LSR above. . Now we train the model with the updated LSR instead and therefore, cross-entropy loss get’s updated to: . . Basically, the new loss H(q′, p) equals 1-Є times the old loss H(q, p) + Є times the cross entropy loss of the noisy labels H(u, p). This is key in understanding Label Smoothing - it is essentially the cross entropy loss with the noisy labels. . Let’s now cut the math and implement this in Microsoft Excel step by step. . Label Smoothing in Microsoft Excel . In this section we implement label smoothing in Microsoft Excel. We know that cross-entropy loss equals: . . Great, and from section-2, we also know that Label Smoothing loss is actually the cross entropy loss with the noisy labels. . Let’s consider we have five images again, but this time of only cats and dogs. . . At the moment, the labels are one-hot encoded. Let’s consider we are using a smoothing factor Є of 0.1. In this case, the updated labels become: . . We get fig-2 by implementing eq-2 on fig-1. So, now we have our LSR labels. Next step is to simply calculate the cross-entropy loss. We will use the fastai implementation of cross-entropy loss in excel, and use it on our LSR labels to calculate the Label Smoothing Cross Entropy Loss. . Let’s consider that our model learns to predict the following logits for each class like so: . . Also, to calculate the cross-entropy loss, we first need to convert the logits to probabilities. The logits are the outputs from the last linear layer of our deep learning model. To convert them to probabilities, we generally have a softmax layer in the end. Jeremy explains how to implement Cross-Entropy loss in Microsoft Excel here including Softmax implementation. . This is the where you PAUSE, look at the video and understand how Jeremy implements Softmax and Cross-Entropy loss in Microsoft Excel. If you already know how, great, let’s move on. . We repeat the same process of applying Softmax operation to the logits to then get our probabilities like so: . . What we have essentially done, is that we take the exponential of the logits, to get exp (cat) and exp (dog) from logit (cat) and logit (dog). Next, we take get the sum (exp) by adding exp (cat) and exp (dog) along the rows. Finally, we get prob (cat) by dividing exp (cat) with sum (exp) and we get prob (dog) by sum (exp). This is how we implement Softmax operation in Microsoft Excel. . So, now that we have successfully converted logits to Probabilities for each image. The next step is simply to calculate the Cross-Entropy loss which from eq-4, is ∑q(x)log(p(x)) where p(x) refers to the predicted probability and q(x) refers to the ground truth label. In our case q(x) are the noisy labels, so, we get the LabelSmoothingCrossEntropy loss like so: . . Believe it or not, we have just successfully implemented Label Smoothing Cross Entropy loss in Microsoft Excel. . Fastai/PyTorch Implementation of Label Smoothing Cross Entropy loss . The Label Smoothing Cross Entropy loss has been implemented in the wonderful fastai library like so: . # Helper functions from fastai def reduce_loss(loss, reduction=&#39;mean&#39;): return loss.mean() if reduction==&#39;mean&#39; else loss.sum() if reduction==&#39;sum&#39; else loss . # Implementation from fastai https://github.com/fastai/fastai2/blob/master/fastai2/layers.py#L338 class LabelSmoothingCrossEntropy(nn.Module): def __init__(self, ε:float=0.1, reduction=&#39;mean&#39;): super().__init__() self.ε,self.reduction = ε,reduction def forward(self, output, target): # number of classes c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction) nll = F.nll_loss(log_preds, target, reduction=self.reduction) # (1-ε)* H(q,p) + ε*H(u,p) return (1-self.ε)*nll + self.ε*(loss/c) . In PyTorch, nn.CrossEntropyLoss() is the same as F.nll_loss(F.log_softmax(...)). Therefore, in the implementation above, nll equates to H(q,p) from eq-3. And then, the loss/c equates to H(u,p) from eq-3 as well where, c equals total number of classes. . For reference again, we know that eq-3 was: . . So, the above implementation can directly be compared to eq-3 and the Label Smoothing Cross Entropy loss then becomes (1-self.ε)*nll + self.ε*(loss/c). . Comparing Microsoft Excel results with PyTorch . Great, now that we know how to implement Label Smoothing Cross Entropy loss in both Microsoft Excel and PyTorch, let’s compare the results. We take the same example as fig-3, and assume that our model in PyTorch predicts the same logits. . # X: model logits or outputs, y: true labels X = torch.tensor([ [4.2, -2.4], [1.6, -0.6], [3.6, 1.2], [-0.5, 0.5], [-0.25, 1.7] ]) y = torch.tensor([0,1,1,0,0]) print(X, &#39; n n&#39;, y) &gt;&gt; #out tensor([[ 4.2000, -2.4000], [ 1.6000, -0.6000], [ 3.6000, 1.2000], [-0.5000, 0.5000], [-0.2500, 1.7000]]) tensor([0, 1, 1, 0, 0]) . This is the same as Microsoft Excel and label 0 corresponds to is_cat and label 1 corresponds to is_dog. Let’s now calculate the Label Smoothing Cross Entropy loss. . LabelSmoothingCrossEntropy(ε=0.1, reduction=&#39;none&#39;)(X,y) &gt;&gt; #out tensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855]) . The results match our Microsoft Excel LS X-entropy results from fig-5. . Conclusion . I hope that through this blog post, I have been able to help you get a thorough understanding of Label Smoothing. By implementing Label Smoothing Cross Entropy loss in Microsoft Excel, step by step, I also hope that I’ve been clear in my attempt to explain everything that goes on behind the scenes. Please feel free to reach out to me via Twitter at @amaarora - constructive feedback is always welcome. . References . A Simple Guide to the Versions of the Inception Network by Bharat Raj | When does label smoothing help by Hinton et al | On Calibration of Modern Neural Networks aka Temperature Scaling by Pleiss et al | Mathematical explainations and proofs for label smoothing by Lei Mao | Label Smoothing + Mixup by Jeremy Howard | Cross Entropy Loss in Microsoft Excel by Jeremy Howard | Credits . This blogpost wouldn’t have been possible without the help of my very talented friend Atmadeep Banerjee. Atmadeep, is currently interning and researching about Instance Segmentation at Harvard! You can find some of his very cool projects at his GitHub here. . Atmadeep was very kind to jump on a call with me for over an hour, when I was unable to replicate the results in Excel and help me find my mistake - LOG function in excel has base 10 whereas in numpy and pytorch it’s LOG to the base e! In MS Excel, LOG to the base e is referred to as LN. . It was really funny to have spent the day reading numerous blog posts, few research papers and source code for PyTorch and then finding out that MS Excel implements LOG function differently than numpy and pytorch. But hey, lesson learnt, when in doubt, contact @Atmadeep Banerjee - he has an eye for detail. .",
            "url": "https://amaarora.github.io/fastexplain/2020/07/18/label-smoothing.html",
            "relUrl": "/2020/07/18/label-smoothing.html",
            "date": " • Jul 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Oganized Pytorch",
            "content": "An introduction to PyTorch Lightning with comparisons to PyTorch . Have you tried PytorchLightning already? If so, then you know why it’s so cool. If you haven’t, hopefully by the time you finish reading this post, you will find it pretty cool (the word ‘it’ could refer to this blogpost or the wonderful PytorchLightning library - I leave this decision to the reader). . Note: From here on, we refer to PytorchLightning as PL, cause it’s a long name to type and I left my favourite keyboard at work. . For a while now, I was jealous of Tensorflow solely because it’s possible to use the same script to train a model on CPU, GPU or TPU without really changing much! For example, take this notebook from my one of my favourite kagglers and - at the time of writing this blogpost - a researcher at NVIDIA - Chris Deotte and also, since yesterday, Kaggle 4x Grandmaster! Just by using an appropriate strategy in Tensorflow, it is possible to run the same experiments on your choice of hardware without changing anything else really. That is the same script could run in TPU, GPU or CPU. . If you’ve already worked on multi-GPU machines or used torch XLA to run things on TPU using PyTorch, then you know my rant. Changing hardware choices in PyTorch is not as convenient when it comes to this. I love PyTorch - I do, but just this one thing would make me really frustrated. . Welcome PL! I wish I tried this library sooner. . In this blogpost, we will be going through an introduction to PL and implement all the cool tricks like - Gradient Accumulation, 16-bit precision training, and also add TPU/multi-gpu support - all in a few lines of code. We use PL to work on SIIM-ISIC Melanoma Classification challenge on Kaggle. In this blogpost, our focus will be on introducing PL and we use the ISIC competition as an example. . We also draw comparisons to the typical workflows in PyTorch and compare how PL is different and the value it adds in a researcher’s life. . The first part of this post, is mostly about getting the data, creating our train and validation datasets and dataloaders and the interesting stuff about PL comes in The Lightning Module section of this post. If this stuff bores you because you’ve done this so many times already, feel free to skip forward to the model implemention. . An introduction to PyTorch Lightning with comparisons to PyTorch What’s ISIC Melanoma Classification challenge? | Getting the data | Melonama Dataset | Lightning Module Model and Training | Model implementation compared to PyTorch | | Gradient Accumulation | 16-bit precision training | TPU Support | Conclusion | Credits | | What’s ISIC Melanoma Classification challenge? . From the description on Kaggle, . Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It’s also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective. Currently, dermatologists evaluate every one of a patient’s moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma. . In this competition, the participants are asked to build a Melonama classifier that classifies to identify melonama in images of skin lesions. Typical lesion images look like the ones below: . . In this blogpost, we will use PL to build a solution that can tell the malign melonama images apart from the rest. The model should take only a few hours to train and have 0.92 AUC score! . A side note: Deep learning has come a far way. Compare this to 2012 where AlexNet was trained on multiple e GTX 580 GPU which has only 3GB of memory. To train on 1.2 million examples of Imagenet, the authors had to split the model (with just 8 layers) to 2 GPUs. It took 5-6 days to train this network. Today, it’s possible to train in a few hours or even minutes. For ISIC, each epoch for size 256x256 is around 2mins including validation on a P100 GPU. . Getting the data . You can download the 256x256 version of the Jpeg images here with all the required metadata to follow along. . Melonama Dataset . Getting our data ready for ingestion into the model is one of the basic things that we need to do for every project. . class MelonamaDataset: def __init__(self, image_paths, targets, augmentations=None): self.image_paths = image_paths self.targets = targets self.augmentations = augmentations def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image_path = self.image_paths[idx] image = np.array(Image.open(image_path)) target = self.targets[idx] if self.augmentations is not None: augmented = self.augmentations(image=image) image = augmented[&#39;image&#39;] return image, torch.tensor(target, dtype=torch.long) . The above dataset is a pretty simple class that is instantiated by passing in a list of image_paths, targets and augmentations if any. To get an item, it reads an image using Image module from PIL, converts to np.array performs augmentations if any and returns target and image. . We can use glob to get train_image_paths and val_image_paths and create train and val datasets respectively. . # psuedo code train_image_paths = glob.glob(&quot;&lt;path_to_train_folder&gt;&quot;) val_image_paths = glob.glob(&quot;&lt;path_to_val_folder&gt;&quot;) sz = 256 #go bigger for better AUC score but slower train time train_aug = train_aug = albumentations.Compose([ RandomCrop(sz,sz), ..., #your choice of augmentations albumentations.Normalize(always_apply=True), ToTensorV2() ]) val_aug = albumentations.Compose([ albumentations.CenterCrop(sz, sz), albumentations.Normalize(always_apply=True), ToTensorV2() ]) train_dataset = MelonamaDataset(train_image_paths, train_targets, train_aug) val_dataset = MelonamaDataset(val_image_paths, val_targets, val_aug) . Once we have our datasets ready, we can now create our dataloaders and let’s inspect the train images as a sanity check. . # Dataloaders train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=64, shuffle=True, num_workers=4) val_loader = torch.utils.data.DataLoader( val_dataset, batch_size=64, shuffle=False, num_workers=4) . # visualize images import torchvision.utils as vutils def matplotlib_imshow(img, one_channel=False): fig,ax = plt.subplots(figsize=(16,8)) ax.imshow(img.permute(1,2,0).numpy()) images= next(iter(train_loader))[0][:16] img_grid = torchvision.utils.make_grid(images, nrow=8, normalize=True) matplotlib_imshow(img_grid) . . Now that our dataloaders are done, and looking good, we are ready for some lightning for our Melonama classifier! . Lightning Module . PL takes away much of the boilerplate code. By taking away the Engineering Code and the Non-essential code, it helps us focus on the Research code! . The Quick Start and Introduction Guide on PL’s official documentation are great resources to start learning about PL! I started there too. . Model and Training . Our model in PL looks something like: . class Model(LightningModule): def __init__(self, arch=&#39;efficientnet-b0&#39;): super().__init__() self.base = EfficientNet.from_pretrained(arch) self.base._fc = nn.Linear(self.base._fc.in_features, 1) def forward(self, x): return self.base(x) def configure_optimizers(self): return torch.optim.Adam(self.parameters(), lr=5e-4) def step(self, batch): x, y = batch y_hat = self(x) loss = WeightedFocalLoss()(y_hat, y.view(-1,1).type_as(y_hat)) return loss, y, y_hat.sigmoid() def training_step(self, batch, batch_nb): loss, y, y_hat = self.step(batch) return {&#39;loss&#39;: loss} def validation_step(self, batch, batch_nb): loss, y, y_hat = self.step(batch) return {&#39;loss&#39;: loss, &#39;y&#39;: y.detach(), &#39;y_hat&#39;: y_hat.detach()} def validation_epoch_end(self, outputs): avg_loss = torch.stack([x[&#39;loss&#39;] for x in outputs]).mean() auc = self.get_auc(outputs) print(f&quot;Epoch {self.current_epoch} | AUC:{auc}&quot;) return {&#39;loss&#39;: avg_loss} def get_auc(self, outputs): y = torch.cat([x[&#39;y&#39;] for x in outputs]) y_hat = torch.cat([x[&#39;y_hat&#39;] for x in outputs]) # shift tensors to cpu auc = roc_auc_score(y.cpu().numpy(), y_hat.cpu().numpy()) return auc . We are using WeightedFocalLoss from my previous blogpost, because this is an imbalanced dataset with only around 1.77% positive classes. . Model implementation compared to PyTorch . We add the __init__ and forward method just like you would in pure PyTorch. The LightningModule just adds some extra functionalities on top. . In pure pytorch, the main loop with training and validation would look something like: . train_dataset, valid_dataset = MelonamaDataset(...), MelonamaDatasaet(...) train_loader, valid_loader = DataLoader(train_dataset, ...), DataLoader(valid_dataset, ...) optimizer = ... scheduler = ... train_augmentations = albumentations.Compose([...]) val_aug = albumentations.Compose([...]) early_stopping = EarlyStopping(...) model = PyTorchModel(...) train_loss = train_one_epoch(model, optimizer, scheduler) preds, valid_loss = evaluate(args, valid_loader, model) report_metrics() if early_stopping.early_stop: save_model_checkpoint() stop_training() . And ofcourse, then we define our train_one_epoch and evaluate functions where the training loop looks typically like: . model.train() for b_idx, data in enumerate(train_loader): loss = model(**data) loss.backward() optimizer.step() optimizer.zero_grad() . And very similar for evaluate. As you can see, we have to write a lot of code to make things work in PyTorch. While this is great for flexibility, typically we have to reuse the same code over and over again in various projects. The training and evaluate loops hardly change much. . What PL does, is that it automates this process for us. No longer do we need to write the boilerplate code. . The training loop, goes directly inside the training_step method and the validation loop inside the validation_step method. The typical reporting of metrics happens inside the validation_epoch_end method. Inside the Model class, both the training_step and validation_step call the step method which get’s the xs and ys from the batch, calls forward to make a forward pass and returns the loss. When we are finished training, our validation loop get’s called and at the end of an epoch validation_epoch_end get’s called which accumulates the results for us and calculates AUC score. We use roc_auc_score because AUC score is used as a metric on the Kaggle competition itself. . And that’s really it. This is all it takes in PL to create, train and validate a deep learning model. There are some other nice functionalities like logging - Wandb and also tensorboard support which you can read more about here. . Shifting from PyTorch to PL is super easy. It took me around a few hours to read up the introduction docs and reimplement the ISIC model in PL. I find PL code is much more organized and compact compared to PyTorch and still very flexible to run experiments. Also, when sharing solutions with others, everybody knows exactly where to look - for example, the training loop is always in the training_step method, validation loop is inside the validation_step and so on. . In some ways, I was able to draw comparisons to the wonderful fastai library in the sense that both the libraries make our lives easier. . Similar to fastai, to train the model in PL, we can now simply create a Trainer and call .fit(). . debug = False gpus = torch.cuda.device_count() trainer = Trainer(gpus=gpus, max_epochs=2, num_sanity_val_steps=1 if debug else 0) trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader) ## outputs &gt;&gt; Epoch 0 | AUC:0.8667878706561116 Epoch 1 | AUC:0.8867006574533746 . And that’s really it. This is all it takes to create a baseline model in PL. . Gradient Accumulation . So now that our baseline model is ready, let’s add gradient accumulation! . trainer = Trainer(gpus=1, max_epochs=2, num_sanity_val_steps=1 if debug else 0, accumulate_grad_batches=2) . It’s as simple as adding a single parameter in PL! . A typical workflow in PyTorch would look like: . accumulate_grad_batches=2 optimizer.zero_grad() for b_idx, data in enumerate(train_loader): loss = model(**data, args=args, weights=weights) loss.backward() if (b_idx + 1) % accumulate_grad_batches == 0: # take optimizer every `accumulate_grad_batches` number of times optimizer.step() optimizer.zero_grad() . PL nicely takes this boilerplate code away from us and provides easy access to researchers to implement gradient accumulation. It is very helpful to have larger batch sizes on a single GPU. To read more about it, refer to this great article by Hugging Face! . 16-bit precision training . 16 bit precision can cut the memory usage by half and also speed up training dramatically. Here is a research paper which provides comprehensive analysis on 16-bit precision training. . For a more gentler introduction refer to the fastai docs here which has some great resources and explains mixed precision very nicely. . To add 16-bit precision training, we first need to make sure that we PyTorch 1.6+. PyTorch only recently added native support for Mixed Precision Training. . To download the latest version of PyTorch simply run . !pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html . After this, adding 16-bit training is as simple as: . trainer = Trainer(gpus=1, max_epochs=2, num_sanity_val_steps=1 if debug else 0, accumulate_grad_batches=2, precision=16) . If you want to continue to use an older version of PyTorch, refer here. . In a typical workflow in PyTorch, we would be using amp fron NVIDIA to directly manipulate the training loop to support 16-bit precision training which can be very cumbersome and time consuming. With PyTorch now adding support for mixed precision and with PL, this is really easy to implement. . TPU Support . Finally, we are down to my last promise of adding TPU support and being able to run this script on TPUs! . Here’s a post by Google introducing TPUs and here is an excellent blogpost comparing various pieces of hardware. TPUs are typically 5 times faster than a V100 and reduce training times significantly. . To use a TPU, switch to Google Colab or Kaggle notebooks with free TPU availability. For more information on TPUs, watch this video by Google again. . To train your models on TPU on PL is again very simple, download the required libraries and add a parameter to the trainer. :) . !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev trainer = Trainer(gpus=1, max_epochs=2, num_sanity_val_steps=1 if debug else 0, accumulate_grad_batches=2, precision=16, tpu_cores=8) . Here is a notebook by Abhishek Thakur for ISIC using TPUs with pure PyTorch. If you compare, you’d realise how easy it is now with PL to train on TPUs. . Conclusion . So I hope by now, you were able to compare the differences between PyTorch and PL and that I have convinced you enough to at least try out PL. [Here] is an excellent Kaggle competition to practice those skills and use PL! In the first few experiments with PL, I have found my work to be more streamlined and also I have noticed a reduction in bugs. I find it easier to experiment with different batch sizes, mixed precision, loss functions, optimizers and also schedulers. PL is definitely worth a try. . Credits . Thanks for reading! And please feel free to let me know via twitter if you did end up trying PyTorch Lightning and the impact this has had on your experimentation workflows. Constructive feedback is always welcome. . The implementation of Model was adapted and modified from this wonderful notebook on Kaggle. | .",
            "url": "https://amaarora.github.io/fastexplain/2020/07/12/oganized-pytorch.html",
            "relUrl": "/2020/07/12/oganized-pytorch.html",
            "date": " • Jul 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Focalloss",
            "content": "What is Focal Loss and when should you use it? . In this blogpost we will understand what Focal Loss and when is it used. We will also take a dive into the math and implement it in PyTorch. . What is Focal Loss and when should you use it? Where was Focal Loss introduced and what was it used for? | So, why did that work? What did Focal Loss do to make it work? | Alpha and Gamma? | How to implement this in code? | Credits | | Where was Focal Loss introduced and what was it used for? . Before understanding what Focal Loss is and all the details about it, let’s first quickly get an intuitive understanding of what Focal Loss actually does. Focal loss was implemented in Focal Loss for Dense Object Detection paper by He et al. . For years before this paper, Object Detection was actually considered a very difficult problem to solve and it was especially considered very hard to detect small size objects inside images. See example below where the model doesn’t predict anything for the motorbike which is of relatively smaller size compared to other images. . . The reason why in the image above, the bike is not predicted by the model is because this model was trained using Binary Cross Entropy loss which really asks the model to be confident about what is predicting. Whereasm, what Focal Loss does is that it makes it easier for the model to predict things without being 80-100% sure that this object is “something”. In simple words, giving the model a bit more freedom to take some risk when making predictions. This is particularly important when dealing with highly imbalanced datasets because in some cases (such as cancer detection), we really need to model to take a risk and predict something even if the prediction turns out to be a False Positive. . Therefore, Focal Loss is particularly useful in cases where there is a class imbalance. Another example, is in the case of Object Detection when most pixels are usually background and only very few pixels inside an image sometimes have the object of interest. . OK - so focal loss was introduced in 2017, and is pretty helpful in dealing with class imbalance - great! . By the way, here are the predictions of the same model when trained with Focal Loss. . . This might be a good time to actually analyse the two and observe the differences. This will help get an intuitive understanding about Focal Loss. . So, why did that work? What did Focal Loss do to make it work? . So now that we have seen an example of what Focal Loss can do, let’s try and understand why that worked. The most important bit to understand about Focal Loss is the graph below: . . In the graph above, the “blue” line represents the Cross Entropy Loss. The X-axis or ‘probability of ground truth class’ (let’s call it pt for simplicity) is the probability that the model predicts for the ground truth object. As an example, let’s say the model predicts that something is a bike with probability 0.6 and it actually is a bike. The in this case pt is 0.6. Also, consider the same example but this time the object is not a bike. Then pt is 0.4 because ground truth here is 0 and probability that the object is not a bike is 0.4 (1-0.6). . The Y-axis is simply the loss value given pt. . As can be seen from the image, when the model predicts the ground truth with a probability of 0.6, the Cross Entropy Loss is still somewhere around 0.5. Therefore, to reduce the loss, our model would have to predict the ground truth label with a much higher probability. In other words, Cross Entropy Loss asks the model to be very confident about the ground truth prediction. . This in turn can actually impact the performance negatively: . The Deep Learning model can actually become overconfident and therefore, the model wouldn’t generalize well. . This problem of overconfidence is also highlighted in this excellent paper Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration. Also, Label Smoothing which was introduced as part of Rethinking the Inception Architecture for Computer Vision is another way to deal with the problem. . Focal Loss is different from the above mentioned solutions. As can be seen from the graph Compare FL with CE, using Focal Loss with γ&gt;1 reduces the loss for “well-classified examples” or examples when the model predicts the right thing with probability &gt; 0.5 whereas, it increases loss for “hard-to-classify examples” when the model predicts with probability &lt; 0.5. Therefore, it turns the models attention towards the rare class in case of class imbalance. . The Focal Loss is mathematically defined as: . . Scary? It’s rather quite intuitive - read on :) . Alpha and Gamma? . So, what the hell are these alpha and gamma in Focal Loss? Also, we will now represent alpha as α and gamma as γ. . Here is my understanding from fig-3: . γ controls the shape of the curve. The higher the value of γ, the lower the loss for well-classified examples, so we could turn the attention of the model more towards ‘hard-to-classify examples. Having higher γ extends the range in which an example receives low loss. . Also, when γ=0, this equation is equivalent to Cross Entropy Loss. How? Well, for the mathematically inclined, Cross Entropy Loss is defined as: . . After some refactoring and defining pt as below: . . Putting eq-3 in eq-2, our Cross Entropy Loss therefore, becomes: . . Therefore, at γ=0, eq-1 becomes equivalent to eq-4 that is Focal Loss becomes equivalent to Cross Entropy Loss. Here is an excellent blogpost that explains Cross Entropy Loss. . Ok, great! So now we know what γ does, but, what does α do? . Another way, apart from Focal Loss, to deal with class imbalance is to introduce weights. Give high weights to the rare class and small weights to the dominating or common class. These weights are referred to as α. . . Adding these weights does help with class imbalance however, the focal loss paper reports: . The large class imbalance encountered during training of dense detectors overwhelms the cross entropy loss. Easily classified negatives comprise the majority of the loss and dominate the gradient. While α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. . What the authors are trying to explain is this: . Even when we add α, while it does add different weights to different classes, thereby balancing the importance of positive/negative examples - just doing this in most cases is not enough. What we also want to do is to reduce the loss of easily-classified examples because otherwise these easily-classified examples would dominate our training. . So, how does Focal Loss deal with this? It adds a multiplicative factor to Cross Entropy loss and this multiplicative factor is (1 − pt)**γ where pt as you remember is the probability of the ground truth label. . From the paper for Focal Loss: . We propose to add a modulating factor (1 − pt)**γ to the cross entropy loss, with tunable focusing parameter γ ≥ 0. . Really? Is that all that the authors have done? That is to add (1 − pt)**γ to Cross Entropy Loss? Yes!! Remember eq-4? . . How to implement this in code? . While TensorFlow provides this loss function here, this is not inherently supported by PyTorch so we have to write a custom loss function. . Here is the implementation of Focal Loss in PyTorch: . class WeightedFocalLoss(nn.Module): &quot;Non weighted version of Focal Loss&quot; def __init__(self, alpha=.25, gamma=2): super(WeightedFocalLoss, self).__init__() self.alpha = torch.tensor([alpha, 1-alpha]).cuda() self.gamma = gamma def forward(self, inputs, targets): BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=&#39;none&#39;) targets = targets.type(torch.long) at = self.alpha.gather(0, targets.data.view(-1)) pt = torch.exp(-BCE_loss) F_loss = at*(1-pt)**self.gamma * BCE_loss return F_loss.mean() . If you’ve understood the meaning of alpha and gamma then this implementation should also make sense. Because, similar to the paper it is simply adding a factor of at*(1-pt)**self.gamma to the BCE_loss or Binary Cross Entropy Loss. . Credits . Please feel free to let me know via twitter if you did end up trying Focal Loss after reading this and whether you did see an improvement in your results! Thanks for reading! . The implementation of Focal Loss has been adapted from here. | fig-1 and fig-2 are from the Fastai 2018 course Lecture-09! | .",
            "url": "https://amaarora.github.io/fastexplain/2020/06/29/FocalLoss.html",
            "relUrl": "/2020/06/29/FocalLoss.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "The Fastai `GetAttr` class",
            "content": "Let&#39;s check the GetAttr source code: . class GetAttr: &quot;Inherit from this to have all attr accesses in `self._xtra` passed down to `self.default`&quot; _default=&#39;default&#39; def _component_attr_filter(self,k): if k.startswith(&#39;__&#39;) or k in (&#39;_xtra&#39;,self._default): return False xtra = getattr(self,&#39;_xtra&#39;,None) return xtra is None or k in xtra def _dir(self): return [k for k in dir(getattr(self,self._default)) if self._component_attr_filter(k)] def __getattr__(self,k): if self._component_attr_filter(k): attr = getattr(self,self._default,None) if attr is not None: return getattr(attr,k) raise AttributeError(k) def __dir__(self): return custom_dir(self,self._dir()) # def __getstate__(self): return self.__dict__ def __setstate__(self,data): self.__dict__.update(data) . The _component_attr_filter checks if the attribute being looked for is not starting with &#39;__&#39; (dunder thingies) or called &#39;_xtra&#39; or self._default. If it&#39;s either of these, return value is False. . Otherwise, if there is some attribute _xtra defined inside the class, then the return is True if k exists in _xtra. OR, return is also True if xtra is None. . Essentially, a call to _component_attr_filter returns True if self._xtra is None or k is part of self._xtra. It returns False if the attribute being looked for is one of dunder thingies or one of self._xtra or self._default. . Finally, when a call to __getattr__ is made, first we check for the attribute self._default if _component_attr_filter returns True. Next, we look for the attribute k being looked inside the self._default attribute thus, passing the getattr to the class&#39;s own attr self._default. . Let&#39;s see an example: . class Robot: name = &quot;IRobot&quot; class Hand(GetAttr): _default = &quot;robot&quot; def __init__(self): self.robot = Robot . hand = Hand() hand.name . &#39;IRobot&#39; . Even the attribute name didn&#39;t exist in Hand class, we still get the value &#39;IRobot&#39; because this class inherits from GetAttr and passes the attribute lookup down to self._default class which in this case is self.robot. . One of the classes that inherits GetAttr is DataLoader and _default is set to dataset which generally is DataSets. . path = untar_data(URLs.MNIST_TINY) dset = Datasets(get_image_files(path)) dl = DataLoader(dset) . Now let&#39;s have a look at dataloaders method. This returns a bound method FilteredBase which actually DataSets inherits from. . As can be seen from the MRO below: . Datasets.__mro__ . (fastai2.data.core.Datasets, fastai2.data.core.FilteredBase, object) . But, this method is made available to DataLoader because _default was set to DataSets and this is due to the magic of GetAttr. . dl.dataloaders . &lt;bound method FilteredBase.dataloaders of (#1428) [(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7565.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7445.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9169.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7924.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7197.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7300.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9024.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9816.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7180.png&#39;),),(Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9515.png&#39;),)...]&gt; .",
            "url": "https://amaarora.github.io/fastexplain/2020/03/31/The-Fastai-GetAttr.html",
            "relUrl": "/2020/03/31/The-Fastai-GetAttr.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Multilabel classification using fastai's `DataBlocks` API",
            "content": "This post is an extension of the previous one on DataBlocks API where we looked at five single image single label classification examples. . In this post, we will look at multilabel classification examples for Computer Vision. This refers to those kinds of problems where a single image can have multiple labels. For example, a challenge could be to recognize common objects present inside an image - in that case the labels would be &quot;desk, table, chair&quot; for a single image with those objects. . As before, we will look at five different multilabel classification examples and build the datablock object for those which is a wrapper that contains - dataloaders, datasets inside fastai. . import fastai2 from fastai2.vision.all import * fastai2.__version__, sys.version . (&#39;0.0.16&#39;, &#39;3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) n[GCC 7.3.0]&#39;) . Example-1: Pascal . As the first example, I would really recommend going over the official documentation here). . Example-2: Freesound Audio Tagging . Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds are difficult to pinpoint. If you close your eyes, could you tell the difference between the sound of a chainsaw and the sound of a blender? . In this example, we will be following @radek&#39;s wonderful notebook on audio classification and apply it to a different dataset. The only difference is unlike Radek&#39;s notebook being a single classification, ours is a multi-label classification - that is, multiple labels can exist for the same audio file. . import librosa import torchaudio import pathlib from IPython.display import Audio . We will be using the curated version Freesound Audio Tagging dataset by Eduardo Fonseca, Manoj Plakal, Frederic Font, Daniel P. W. Ellis, Xavier Serra. Recently, a kaggle competition for the same was also held. . Note that untar_data can be used to get data for any URL not just fastai URLs. . path = untar_data(&quot;https://zenodo.org/record/3612637/files/FSDKaggle2019.audio_train_curated.zip&quot;) . path.ls() . (#2) [Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/labels&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files&#39;)] . def get_audio_files(path): return get_files(path, extensions=&#39;.wav&#39;) . We will be using the fastai&#39;s convenience function get_files that is used get a path of all the files. It is also possible to pass extensions to the function and in that case, the function only returns those files that have extensions that are part of those passed to the method. We will be passing .wav as extension to get all .wav files. . wav_files = get_files(path, extensions=&#39;.wav&#39;); wav_files . (#4970) [Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/ae628bb7.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/f291b5f4.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/0f5c8134.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/749a39f0.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/368e95b1.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/ae8110df.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/0e396c69.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/4bf171d7.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/7730996a.wav&#39;),Path(&#39;/home/ubuntu/.fastai/data/FSDKaggle2019.audio_train_curated/audio_files/8f42bea5.wav&#39;)...] . Audio(filename=str(wav_files[2])) . Your browser does not support the audio element. Can you guys hear traffic noise? Great! :) . The labels can also be downloaded from the same data source, and are present in the train_curated_post_competition.csv file. . labels = pd.read_csv(path/&#39;labels&#39;/&#39;train_curated_post_competition.csv&#39;) labels.set_index(&quot;fname&quot;, inplace=True) labels[&#39;labels&#39;] = labels.labels.str.replace(&quot;,&quot;, &quot; &quot;) labels.head(10) . labels freesound_id license . fname . ac9e7a91.wav Church_bell | 65579 | CC-BY-NC | . 65ae847e.wav Frying_(food) | 65583 | CC Sampling+ | . 32ec2454.wav Computer_keyboard | 360502 | CC0 | . af7b5bab.wav Scissors | 360503 | CC0 | . 7e8cd849.wav Purr | 65598 | CC-BY | . fdfbf113.wav Gasp | 98394 | CC-BY | . 130aa63f.wav Applause Cheering | 32863 | CC-BY-NC | . db198721.wav Applause Crowd Cheering | 32864 | CC-BY-NC | . 492c192f.wav Applause Cheering | 32865 | CC-BY-NC | . 586ab251.wav Applause Cheering | 32866 | CC-BY-NC | . Some of the files such as 492c192f.wav have multiple labels such as Applause and Cheering and therefore this is a multilabel classification problem. . In our get_x we create a spectogram image of the Audio so that we can use a cnn_learner to classify spectograms instead of classifying audio files. This has been a trick that has been used to get state of art results in many audio competitions. . def get_x(path, target_rate=44100, num_samples=int(66150)): x, rate = torchaudio.load_wav(path, normalization=lambda x: torch.abs(x).max()) x = x[0] / 500 x = x.numpy().squeeze() x = librosa.util.fix_length(x, num_samples) spec = librosa.feature.melspectrogram(x, sr=rate, n_fft=1024, hop_length=140) return spec.astype(np.uint8) . For the get_y function we return the labels as a list which will be passed to the MultiCategoryBlock to convert them to ints that represent the category. . def get_y(path, df=labels): return df.loc[path.name][&#39;labels&#39;].split() . We create the DataBlock object as before in the first post. . dblock = DataBlock( blocks=(ImageBlock, MultiCategoryBlock), get_items=get_audio_files, get_x=get_x, get_y=get_y, splitter=RandomSplitter() ) . dls = dblock.dataloaders(path) . As we can see below, the problem now simply becomes an image classification problem. . dls.show_batch() . learn = cnn_learner(dls, resnet34, metrics=[accuracy_multi]) learn.fine_tune(6) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.927335 | 0.562045 | 0.725578 | 01:18 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.624793 | 0.407948 | 0.904867 | 01:20 | . 1 | 0.372609 | 0.118691 | 0.984708 | 01:18 | . 2 | 0.184834 | 0.072461 | 0.985702 | 01:20 | . 3 | 0.114148 | 0.065702 | 0.985978 | 01:22 | . 4 | 0.086914 | 0.062889 | 0.986104 | 01:22 | . 5 | 0.078180 | 0.062337 | 0.986079 | 01:21 | . Very quickly we are able to get to an astonishing multi label accuracy of 98.6%! . Example-3: Atlas Protein Classification . Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells. . For the second example, we will be looking at the Human Protein Atlas Image Classification kaggle competition. Akash Palrecha, Sanyam Bhutani and Rekil Prashanthand I worked together on this example. . The primary challenge with this competition is how the data is presented. The images are four channel and each image is a separate .png file. There are two ways to work in this competition, either concat all four channels together to create a four channel image or select only three channels (randomly or first three), to create a channel image. The below code was run for three channel images - but, just uncomment the &#39;yellow&#39; in concat_four_channel function and replace the cnn_learner with get_model function and this same code will work for four channel images inside fastai. . We will using cv2 to read .png files and concat them together to a three/four channel images. . import cv2 . We use the usual get_image_files to get the .png files and as we can see in the top-4 files, each of the file has id 00070df0-bbc3-11e8-b2bc-ac1f6b6435d0 followed by _blue, _green, _yellow and _red. . path = Path(&quot;/home/ubuntu/repos/kaggle/atlas/data/&quot;) files = get_image_files(path/&#39;train&#39;) files.sort() files[:4] . (#4) [Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_blue.png&#39;),Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_green.png&#39;),Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_red.png&#39;),Path(&#39;/home/ubuntu/repos/kaggle/atlas/data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0_yellow.png&#39;)] . To create a four channel image, we will have to pass all 4 images to the get_x read each of blue, green, red and yellow images. We do this py passing every 4th image in a sorted list, extract the image_id from the image path passed, and read other three images also. Finally we concat the images and return the four channel images. . def get_4th_item(path): files = get_image_files(path) files.sort() return files[slice(0, len(files), 4)] . The below function concat_4_channel takes in a unique image path, extracts the image_id from the path and reads the other three images as well to create and return a four channel PILImage. . def concat_4_channel(fname): fname = str(fname) if fname.endswith(&#39;.png&#39;) or fname.endswith(&#39;.tif&#39;): suffix = fname[-4:] fname = fname.split(&#39;_&#39;)[0] colors = [&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;, &#39;yellow&#39;] flags = cv2.IMREAD_GRAYSCALE img = [cv2.imread(fname+&#39;_&#39;+color+&#39;.png&#39;, flags).astype(np.uint8) for color in colors] x = np.stack(img, axis=-1) return PILImage.create(x) . Finally, let&#39;s read in the labels. The labels are provided in a separate train.csv file and this problem is a multilabel classification one. A single image can have multiple protein strands. . df = pd.read_csv(path/&quot;train.csv&quot;) df.set_index(&quot;Id&quot;, inplace=True) df.head() . Target . Id . 00070df0-bbc3-11e8-b2bc-ac1f6b6435d0 16 0 | . 000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0 7 1 2 0 | . 000a9596-bbc4-11e8-b2bc-ac1f6b6435d0 5 | . 000c99ba-bba4-11e8-b2b9-ac1f6b6435d0 1 | . 001838f8-bbca-11e8-b2bc-ac1f6b6435d0 18 | . To get the labels inside the DataBlock, we will pass the unique image_id to the dataframe, index into it and get the Target labels. The below function does exactly the same. . def get_labels_from_df(path, df=df): if not isinstance(path, Path): path = Path(path) path = path.stem path = path[:-5] return df.loc[path].values[0].split(&quot; &quot;) . Now we are all set to create the DataBlock object. We pass the get_4th_item in get_items, concat_4_channel as get_x and get_labels_from_df as get_y. . dblock = DataBlock( blocks=(ImageBlock(cls=PILImageBW), MultiCategoryBlock), get_items=get_4th_item, get_x=concat_4_channel, get_y=get_labels_from_df ) . Finally, we call the dataloaders method to create the dataloaders. Remember, the DataBlock is a frame and calling the dataloaders method is when everything is run. . dls = dblock.dataloaders(path/&#39;train/&#39;, bs=32) . Finally, we can call the show_batch function to check the batch and labels. . dls.show_batch() . Excellent, now we are ready to train the learner and make predictions on the test set. . But, we also need to make sure that the model is able to accept four channel images. . def get_model(pretrained=True): m = resnet34(pretrained) wt = m.conv1.weight conv = nn.Conv2d(4,64,7,stride=2,padding=3) weight = conv.weight weight[:,3] = wt.data[:,0].clone() conv.weight = nn.Parameter(weight) m.conv1 = conv return m . Let&#39;s use the competition metric to get a performance estimate of the model. . f1_score = F1ScoreMulti(thresh=0.3) . learn = cnn_learner(dls, resnet34, metrics=[f1_score]) # learn = cnn_learner(dls, get_model, metrics=[f1_score]) . Great, now let&#39;s learn the learning rate finder to get an appropriate learning rate. . learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.04786301031708717) . learn.fine_tune(15, base_lr=1e-3) . epoch train_loss valid_loss f1_score time . 0 | 0.175298 | 0.147654 | 0.130897 | 02:14 | . epoch train_loss valid_loss f1_score time . 0 | 0.145895 | 0.129687 | 0.217130 | 03:03 | . 1 | 0.131300 | 0.117884 | 0.281944 | 03:03 | . 2 | 0.119659 | 0.108711 | 0.339926 | 03:03 | . 3 | 0.103923 | 0.103715 | 0.387413 | 03:03 | . 4 | 0.092060 | 0.102526 | 0.423547 | 03:03 | . 5 | 0.073156 | 0.105685 | 0.440281 | 03:03 | . 6 | 0.053137 | 0.115068 | 0.435095 | 03:03 | . 7 | 0.036532 | 0.125479 | 0.442718 | 03:03 | . 8 | 0.024828 | 0.137905 | 0.454886 | 03:03 | . 9 | 0.017110 | 0.143491 | 0.462277 | 03:03 | . 10 | 0.012493 | 0.148489 | 0.445136 | 03:03 | . 11 | 0.008337 | 0.150278 | 0.459201 | 03:03 | . 12 | 0.006240 | 0.154228 | 0.455904 | 03:03 | . 13 | 0.005097 | 0.154765 | 0.456672 | 03:03 | . 14 | 0.004447 | 0.154457 | 0.453888 | 03:03 | . learn.fine_tune(5, base_lr=2e-2) . epoch train_loss valid_loss f1_score time . 0 | 0.046586 | 0.218287 | 0.386139 | 02:16 | . epoch train_loss valid_loss f1_score time . 0 | 0.117914 | 0.115358 | 0.349613 | 03:03 | . 1 | 0.110822 | 0.100517 | 0.425270 | 03:03 | . 2 | 0.094380 | 0.092146 | 0.456394 | 03:03 | . 3 | 0.069953 | 0.084606 | 0.514543 | 03:03 | . 4 | 0.048090 | 0.087329 | 0.536810 | 03:03 | . learn.save(&quot;model.bin&quot;) . test_files = get_4th_item(path/&quot;test&quot;) . dl = learn.dls.test_dl(test_files, bs=64) . preds, _ = learn.get_preds(dl=dl) . thresh = 0.3 labelled_preds = [&#39; &#39;.join([learn.dls.vocab[i] for i,p in enumerate(pred) if p &gt; thresh]) for pred in preds.numpy()] . sub = pd.DataFrame({ &#39;Id&#39; : [item[:-5] for item in test_files.attrgot(&quot;stem&quot;)], &#39;Predicted&#39;: labelled_preds }) . sub.head() . Id Predicted . 0 00008af0-bad0-11e8-b2b8-ac1f6b6435d0 | 2 | . 1 0000a892-bacf-11e8-b2b8-ac1f6b6435d0 | 5 | . 2 0006faa6-bac7-11e8-b2b7-ac1f6b6435d0 | 0 25 5 | . 3 0008baca-bad7-11e8-b2b9-ac1f6b6435d0 | 0 25 | . 4 000cce7e-bad4-11e8-b2b8-ac1f6b6435d0 | 23 | . sub.to_csv(&quot;submission.csv&quot;, index=False) . !kaggle competitions submit -c human-protein-atlas-image-classification -f submission.csv -m &quot;fastai basic&quot; . 100%|████████████████████████████████████████| 472k/472k [00:09&lt;00:00, 50.5kB/s] Successfully submitted to Human Protein Atlas Image Classification . Conclusion . In this blogpost, we have had a look at how to get multilabel predictions from the fastai library outside the official example of Pascal. The first example followed @radek&#39;s wonderful notebook on audio classification and we applied it to a different dataset. . Next, inside the second example, we had a look at the Protein classification problem and also saw how to read four channel images inside the library. . Credits . Special thanks to Radek Osmulski for the wonderful Audio classification notebook and sharing it with everyone on the Fastai Forums. . Also, I want to thank Akash Palrecha, Sanyam Bhutani and Rekil Prashanth for working on the second example together. .",
            "url": "https://amaarora.github.io/fastexplain/2020/03/26/DataBlocks-API-multi-label-classification.html",
            "relUrl": "/2020/03/26/DataBlocks-API-multi-label-classification.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "DataBlocks API - A high level introduction with examples",
            "content": "Fastai&#39;s very popular Part-1 Deep Learning for coders course has begun this week from March 17, 2020. I am honored to be a part of the group that get&#39;s to do it &quot;live&quot; before the course get&#39;s released to the general public in July. . A high level introduction to the DataBlocks can be found here while the official DataBlocks documentation can be referenced here. . In general the DataBlocks API is an easy-to-use, highly flexible and a very powerful API that can be used to build DataLoaders and Datasets for various different Deep Learning applications such as VISION, TEXT, TABULAR and COLLABORATION. . In this post, we will be focussing on VISION applications and learn to use the DataBlocks API for 5 different vision based single label deep learning applications. . We will be following the top-down approach as in Fastai and learn about the DataBlocks API by &quot;doing rather than reading&quot;. . We will be using Kaggle for 5 different single label CV applications (each one a little bit different from the other) and build a DataBlock for each of these competitions. In this post the focus will be on getting the data in, rather than model training. . In the world of deep learning, getting the data ready for training is an essential (usually time consuming) step and we will be focusing on this rather than model training in this blog post by making use of the DataBlocks API. . # using version 0.0.14 of fastai2 import fastai2 from fastai2.vision.all import * fastai2.__version__, sys.version . (&#39;0.0.14&#39;, &#39;3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) n[GCC 7.3.0]&#39;) . Example-1: MNIST . The dataset for this example consists of image files that are split by folders. Example referenced from the fastai documentation here. . Let&#39;s start with the hello-world example of deep learning for handwritten digit classification. We will be building the DataBlocks API for a smaller version of this dataset but this will also hold for the complete version. . path = untar_data(URLs.MNIST_TINY) . # !sudo apt-get install tree !tree -d {path} . /home/ubuntu/.fastai/data/mnist_tiny ├── models ├── test ├── train │   ├── 3 │   └── 7 └── valid ├── 3 └── 7 8 directories . As we can see from the folder structure above, we have train, test and valid folders. Each of train and valid folders have 3 and 7 subfolders with images while test consists of some image files. . files = get_image_files(path) . get_image_files is a convenience function that returns the paths of all image files that are available inside the path tree passed as an argument. . files . (#1428) [Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7565.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7445.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9169.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7924.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7197.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7300.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9024.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9816.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7180.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9515.png&#39;)...] . As a general concept, we need to tell the DataBlock API 4 things: . What kind of problem we&#39;re working on? | How to get the items? | How to label the items? | How to create the validation set? | dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=GrandparentSplitter()) . As a DataBlock introduction, the four things that we pass to build our DataBlock object are: . blocks: Generally, two blocks are always passed for each different example - one, for the X and another for the y variable. Since, in this case, our Xs are images, the X block becomes ImageBlock and since this is single category classification, our y block becomes CategoryBlock. The blocks share the information regarding the type of problem we&#39;re working on. . get_items: Next, we need to pass a function to the API that get&#39;s applied on the source passed to the dblock.dataloaders(source) method. This function tells the DataBlock API on how to get the items. . files = get_image_files(path); files . (#1428) [Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7565.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7445.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9169.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7924.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7197.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7300.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9024.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9816.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/7180.png&#39;),Path(&#39;/home/ubuntu/.fastai/data/mnist_tiny/valid/7/9515.png&#39;)...] . As can be seen from above example, get_image_files returns a L (similar but more powerful than the Python List in fastai) of items which are Paths to various image files. . get_y: The get_y function get&#39;s applied to each of the files returned after get_items is applied to source. This function returns the y variable from these paths. In our case, we pass the parent_label which returns the parent label of a function (the folder name) which becomes the y variable. This works because of the way our data is set up. . splitter: The last thing that we pass to the DataBlocks API is a splitter. The splitter get&#39;s applied to each of the items returned by get_items function to split the data into train and valid set. Since, our data is already split into train and valid folders, we pass GrandParentSplitter as the splitter function which splits the data into train and valid based on grand parent folder name. . dls = dblock.dataloaders(path) . dls.train_ds[0] . (PILImage mode=RGB size=28x28, TensorCategory(1)) . dls.show_batch() . Example-2: PETS . All images for the PETS dataset exist in a single directory but the labels can be extracted fro filenames themselves. Example referenced from the fastai documentation here. . path = untar_data(URLs.PETS)/&#39;images&#39;; path . Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images&#39;) . # !sudo apt-get install tree !tree -d {path} . /home/ubuntu/.fastai/data/oxford-iiit-pet/images 0 directories . Since, we have 0 directories, it means all image files are part of this one directory /home/ubuntu/.fastai/data/oxford-iiit-pet/images. This path might be different on your machine.. . Also, the y labels need to be extracted from the image names themselves. This is different from the folder structure that we saw in MNIST. . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(224,224)), get_items=get_image_files, get_y=[attrgetter(&quot;name&quot;), RegexLabeller(pat=r&#39;^(.*)_ d+.jpg$&#39;)], splitter=RandomSplitter()) . Because the objective here is to do single label classification again, therefore, we pass the ImageBlock and CategoryBlock as the X and y block again. . We also pass item_tfms which randomly crops and resizes each of the images to a 224*224 size. We need this for model training - in deep learning, we require that our image sizes are the same. As the name suggests item_tfms get applied to each X item, therefore, each image is randomly cropped and resized to 224*224. . Our get_items function is the same as before. . For get_y since, we need to extract images from the Path, first we get the name attribute of each of the paths returned by get_items, next we use a RegexLabeller to extract the label from the names. . files = get_image_files(path) files[:10] . (#10) [Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Siamese_178.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/german_shorthaired_94.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Abyssinian_92.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/basset_hound_111.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_194.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_91.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Persian_69.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/english_setter_33.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/Russian_Blue_155.jpg&#39;)] . names = files[:10].attrgot(&quot;name&quot;) names . (#10) [&#39;keeshond_34.jpg&#39;,&#39;Siamese_178.jpg&#39;,&#39;german_shorthaired_94.jpg&#39;,&#39;Abyssinian_92.jpg&#39;,&#39;basset_hound_111.jpg&#39;,&#39;Russian_Blue_194.jpg&#39;,&#39;staffordshire_bull_terrier_91.jpg&#39;,&#39;Persian_69.jpg&#39;,&#39;english_setter_33.jpg&#39;,&#39;Russian_Blue_155.jpg&#39;] . labeller = RegexLabeller(pat=r&#39;^(.*)_ d+.jpg$&#39;) labels = names.map(labeller) labels . (#10) [&#39;keeshond&#39;,&#39;Siamese&#39;,&#39;german_shorthaired&#39;,&#39;Abyssinian&#39;,&#39;basset_hound&#39;,&#39;Russian_Blue&#39;,&#39;staffordshire_bull_terrier&#39;,&#39;Persian&#39;,&#39;english_setter&#39;,&#39;Russian_Blue&#39;] . The above three steps explain how we extract labels from our paths that were returned by the get_items function. . dls = dblock.dataloaders(path) . dls.show_batch() . Example-3: Google Landmark . Google Landmark Recognition 2019 kaggle competition to predict the landmark shown in the image. . Since the complete dataset size for this competition is around 100 gb, we will only be working on a subset of the dataset. . path = download_data(&quot;https://s3.amazonaws.com/google-landmark/train/images_000.tar&quot;) path . Path(&#39;/home/ubuntu/.fastai/archive/images_000.tar&#39;) . So the .tar file get&#39;s downloaded at home/ubuntu/.fastai/archive/images_000.tar at my machine. download_data is a convenience function inside fastai and more about it can be found here. . #untar file !tar -xf {path} . !tree -d {path.parent/&#39;0&#39;} . /home/ubuntu/.fastai/archive/0 └── 0 ├── 0 ├── 1 ├── 2 ├── 3 ├── 4 ├── 5 ├── 6 ├── 7 └── 8 10 directories . In this competition, all images are present in the path (similar to PETS) but the labels are provided in a .csv file. This is different from PETS where we built the labels from the path names themselves. . Let&#39;s download that file. From the competition information available here the train .csv file can be downloaded from https://s3.amazonaws.com/google-landmark/metadata/train.csv. . trn_csv_pth = download_data(&quot;https://s3.amazonaws.com/google-landmark/metadata/train.csv&quot;) trn_csv_pth . Path(&#39;/home/ubuntu/.fastai/archive/train.csv&#39;) . trn_labels = pd.read_csv(trn_csv_pth) trn_labels.head() . id url landmark_id . 0 6e158a47eb2ca3f6 | https://upload.wikimedia.org/wikipedia/commons/b/b5/Observatoriet_v%C3%A4derkammer_2013a.jpg | 142820 | . 1 202cd79556f30760 | http://upload.wikimedia.org/wikipedia/commons/6/63/Ecosse200996-1.jpg | 104169 | . 2 3ad87684c99c06e1 | http://upload.wikimedia.org/wikipedia/commons/2/2c/Pirmasens_Dynamikum.jpg | 37914 | . 3 e7f70e9c61e66af3 | https://upload.wikimedia.org/wikipedia/commons/0/02/Occidental_Vertical.jpg | 102140 | . 4 4072182eddd0100e | https://upload.wikimedia.org/wikipedia/commons/5/51/Looking_downstream_from_the_footbridge_over_the_Severn_-_geograph.org.uk_-_532337.jpg | 2474 | . Since we don&#39;t need the url column, let&#39;s drop it. . trn_labels.drop(&quot;url&quot;, axis=1, inplace=True) . So in this competition, the images are named as id.jpg from the dataframe above and the labels are inside the df as well. This is different from both the PETS and MNIST data structure. . files = get_image_files(path.parent/&#39;0&#39;) . files . (#8266) [Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/00562e33f481adc5.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0054a9a95ff4190c.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0054318893cd2ea8.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0059b37f9ea1e443.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/00509f87e2140fe8.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0059cc88f2dbb53a.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/005303e80b4f14a2.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/0050051bd1831856.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/00575e870647b87c.jpg&#39;),Path(&#39;/home/ubuntu/.fastai/archive/0/0/5/005959682da86fa8.jpg&#39;)...] . So now we have a list of 8266 image files and since this is only a subset of the dataset, we need to only keep those rows in the trn_labels dataframe for which we have images. The following step wouldn&#39;t be necessary if you&#39;re working with the complete dataset. . fnames = files.attrgot(&#39;stem&#39;) fnames . (#8266) [&#39;00562e33f481adc5&#39;,&#39;0054a9a95ff4190c&#39;,&#39;0054318893cd2ea8&#39;,&#39;0059b37f9ea1e443&#39;,&#39;00509f87e2140fe8&#39;,&#39;0059cc88f2dbb53a&#39;,&#39;005303e80b4f14a2&#39;,&#39;0050051bd1831856&#39;,&#39;00575e870647b87c&#39;,&#39;005959682da86fa8&#39;...] . trn_labels = trn_labels[trn_labels[&quot;id&quot;].isin(fnames)].reset_index(drop=True) trn_labels.set_index(&quot;id&quot;, inplace=True) trn_labels.head() . landmark_id . id . 0036d78c05c194d9 50089 | . 001cd787f1e9a803 61937 | . 00429b0a692bc6ec 183170 | . 0082fd4214b3c2c7 36407 | . 002b386016930458 119649 | . Now we are ready to create our DataBlock object. . def get_label_from_df(path, df=trn_labels): id = path.stem return df.loc[id].values[0] . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms = RandomResizedCrop(size=(224,224)), get_items=get_image_files, get_y=get_label_from_df, splitter=RandomSplitter()) . dls = dblock.dataloaders(path.parent/&#39;0&#39;) . dls.show_batch() . Example-4: Distracted Driver Detection . In this competition you are given driver images, each taken in a car with a driver doing something in the car (texting, eating, talking on the phone, makeup, reaching behind, etc). Your goal is to predict the likelihood of what the driver is doing in each picture. . data_path = Path(&quot;/home/ubuntu/repos/kaggle/driver/data/&quot;) . !tree -d {data_path} . /home/ubuntu/repos/kaggle/driver/data └── imgs ├── test └── train ├── c0 ├── c1 ├── c2 ├── c3 ├── c4 ├── c5 ├── c6 ├── c7 ├── c8 └── c9 13 directories . All images are split into folders similar to the PETS data structure. But, for this type of competition, we won&#39;t be using the DataBlocks API similar to PETS. The reason wil be explained in a moment. . We will use the same get_image_files as before, to read in the files from the data_path. . files = get_image_files(data_path/&quot;imgs&quot;) . files . (#102150) [Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_31511.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_63076.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_27600.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_31852.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_98339.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_68938.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_85715.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_41593.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_78475.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/driver/data/imgs/test/img_99303.jpg&#39;)...] . Also, an interesting thing here is that we have around ~20K train images while we have ~80K test images. . Similar to the Google Landmark Images competition from before, the labels are given in a dataframe while the images are available in the train folder. We will build the DataBlocks API a little differently than we did in the last competition. This time we will instead use the pandas dataframe, to get_x and get_y labels. . We use the dataframe instead of splitting by folders, because with the dataframe we can pass the valid_idx values (index values for the validation set). From the Kaggle competition here we can see that the test set consists of driver images of those drivers that were not part of the train set. . Therefore, we would like our validation set to consist of drivers not part of the train set as well. This is possible by passing in validation index values such that subject in the train dataframe are different in validation set from those in train set. . trn_labels = pd.read_csv(data_path/&quot;driver_imgs_list.csv&quot;) trn_labels.head() . subject classname img . 0 p002 | c0 | img_44733.jpg | . 1 p002 | c0 | img_72999.jpg | . 2 p002 | c0 | img_25094.jpg | . 3 p002 | c0 | img_69092.jpg | . 4 p002 | c0 | img_92629.jpg | . def get_paths_from_df(df_values): path = Path(data_path)/&quot;imgs&quot;/&quot;train&quot;/df_values[1]/df_values[2] return PILImage.create(path) . In this competition, the idea is to predict the classname rather than the subject so we will use this as our y_label. . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(224,224)), get_x=get_paths_from_df, splitter=IndexSplitter(trn_labels[-1983:].index), get_y=ColReader(1) ) . The DataBlock API is built differently than the Google Landmark Detection challenge where we used a RandomSplitter, got our Xs from paths, while ys from dataframe. . Instead, in this competition, we use an IndexSplitter to make sure that the valid set consists of drivers not part of train set. . We also use get_paths_from_df as our get_x. Essentially, all that this function does is to generate a path of our image based on the trn_labels dataframe values. Once we get the path from get_x, we use PILImage.create method to return a PIL Image. This image becomes our X. . To get the y variable we simply read the &quot;classname&quot; column of the dataframe. . dls = dblock.dataloaders(trn_labels.values) . dls.show_batch() . Now, getting these c0, c1 class names are not very helpful. Let&#39;s instead get the actual image captions. We can do this by simply adding a get_true_label function which returns the true label based on a dictionary defined in the Kaggle competition. . def get_true_label(key): return dict( c0= &#39;safe driving&#39;, c1= &#39;texting - right&#39;, c2= &#39;talking on the phone - right&#39;, c3= &#39;texting - left&#39;, c4= &#39;talking on the phone - left&#39;, c5= &#39;operating the radio&#39;, c6= &#39;drinking&#39;, c7= &#39;reaching behind&#39;, c8= &#39;hair and makeup&#39;, c9= &#39;talking to passenger&#39; )[key] . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(256, 256)), get_x=get_paths_from_df, splitter=IndexSplitter(trn_labels[-1983:].index), get_y=[ColReader(1), get_true_label] ) . We add the get_true_label function to the get_y and now the get_y becomes a list of functions which will be applied serially. The first function ColReader(1) will read the second column and return the &quot;classname&quot; such as c0, c1.. and next, get_true_label will return a true label based on the dictionary and key. The key passed to get_true_label is the output of ColReader(1) therefore, one of c0, c1, c2.. . Let&#39;s build the DataBlock object again and check the dataloader batch. . dls = dblock.dataloaders(trn_labels.values) . dls.show_batch() . Example-5: Plant Pathology 2020 - FGVC7 . Given a photo of an apple leaf, can you accurately assess its health? This competition will challenge you to distinguish between leaves which are healthy, those which are infected with apple rust, those that have apple scab, and those with more than one disease. . data_path = Path(&quot;/home/ubuntu/repos/kaggle/plant/data/&quot;) . !tree -d {data_path} . /home/ubuntu/repos/kaggle/plant/data └── images 1 directory . All images are inside the images directory. Let&#39;s use the get_image_files function to have a look at them. . files = get_image_files(data_path/&#39;images&#39;) files . (#3642) [Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_116.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_542.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_798.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Train_159.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_61.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_713.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_1387.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Train_556.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Test_551.jpg&#39;),Path(&#39;/home/ubuntu/repos/kaggle/plant/data/images/Train_499.jpg&#39;)...] . Each label path name consists of Test or Train labels (which we can use for splitting). The Test files do not have any labels because this the test set on which we need to submit predictions to Kaggle. . Therefore, we can use the Train set to read images, do a random split using the RandomSpliter() to create a random valid dataset and finally run predictions on the test set. . trn_labels = pd.read_csv(data_path/&quot;train.csv&quot;) . image_id healthy multiple_diseases rust scab . 0 Train_0 | 0 | 0 | 0 | 1 | . 1 Train_1 | 0 | 1 | 0 | 0 | . 2 Train_2 | 1 | 0 | 0 | 0 | . 3 Train_3 | 0 | 0 | 1 | 0 | . 4 Train_4 | 1 | 0 | 0 | 0 | . Also, the labels are one-hot encoded inside the DataFrame. Therefore, to make the DataBlocks API simpler, I believe it&#39;s best to convert the one-hot-encoded variables to integers. . cats = [np.where(r==1)[0][0] for r in trn_labels[[&#39;healthy&#39;, &#39;multiple_diseases&#39;, &#39;rust&#39;, &#39;scab&#39;]].values] trn_labels[&#39;cat&#39;] = cats . trn_labels.head() . image_id healthy multiple_diseases rust scab cat . 0 Train_0 | 0 | 0 | 0 | 1 | 3 | . 1 Train_1 | 0 | 1 | 0 | 0 | 1 | . 2 Train_2 | 1 | 0 | 0 | 0 | 0 | . 3 Train_3 | 0 | 0 | 1 | 0 | 2 | . 4 Train_4 | 1 | 0 | 0 | 0 | 0 | . def get_true_label(key): return { 0: &#39;healthy&#39;, 1: &#39;multiple_diseases&#39;, 2: &#39;rust&#39;, 3: &#39;scab&#39; }[key] . The get_x function reader the first column of the trn_labels dataframe above, adds a &quot;data_path/images/&quot; prefix to it and a .jpg suffix, therefore, converting it to a path. . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), item_tfms=RandomResizedCrop(size=(224, 224)), get_x=ColReader(0, pref=f&quot;{data_path}/images/&quot;, suff=&quot;.jpg&quot;), splitter=RandomSplitter(), get_y=[ColReader(5), get_true_label] ) . We call the dataloaders as before to create the dataloaders. . dls = dblock.dataloaders(trn_labels.values) . dls.show_batch() . Summary . So, in this post, we specifically looked at single image, single label type Vision Classification problems and looked at different ways of using the DataBlocks API to build the DataBlock object and dataloaders. . In the coming few days, we will look at the DataBlocks API in more detail, use the same API structure to build dataloaders for Single Image, Multi Label type classification, text classification and also look at an interesting example on how to deal with Multi Image, Single Label classification type problem. . In the future posts, we will also start digging deeper in to the implementation details of the DataBlocks API and the fastai library as a whole. . Credits . Thanks to @vijayabhaskar for pointing out that it would be easier to build the DataBlocks API in the fifth example using ColReader in get_x. | Thanks to @muellerzr for pointing out a typo in the blog. |",
            "url": "https://amaarora.github.io/fastexplain/2020/03/24/DataBlocks-API.html",
            "relUrl": "/2020/03/24/DataBlocks-API.html",
            "date": " • Mar 24, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Annotatedgpt2",
            "content": "The Annotated GPT-2 . The Annotated GPT-2 Introduction | Prerequisites | Language Models are Unsupervised Multitask Learners | Abstract | Model Architecture (GPT-2) | Model Specifications (GPT) | Imports | Transformer Decoder inside GPT-2 CONV1D Layer Explained | FEEDFORWARD Layer Explained | ATTENTION Layer Explained Scaled Dot-Product Attention | Multi-Head Attention | | | GPT-2 Model Architecture in Code Transformer Decoder Block Explained | | The GPT-2 Architecture Explained Language Modeling or Classification | | Sample text generation using Hugging Face Pretrained Weights | Extras | Credits | Feedback | | Introduction . Welcome to “The Annotated GPT-2”. . One of the most brilliant and well-explained articles I have ever read is The Annotated Transformer. It introduced Attention like no other post ever written. The simple idea was to present an “annotated” version of the paper Attention is all you need along with code. . Something I have come to realize with my little experience in Machine Learning, when you write things in code, the implementation and the secrets become clearer. It is not magic anymore. . There is nothing magic about magic. The magician merely understands something simple which doesn’t appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can “do magic.” . – Jeffrey Friedl in the book Mastering Regular Expressions . The GPT-2 might seem like magic at first with all it’s glitter and beauty too, but hopefully I would have uncovered that magic for you and revealed all the tricks by the time you finish reading this post. That is my goal. To make it as simple as possible for the keen to understand how the GPT-2 model works underneath. . Note: Pretty much the entirety of the code has been copied, inspired and referenced from Hugging Face’s implementation of the GPT-2, keeping merely the essentials for simplicity. If you want to train the GPT-2 model on parallel GPUs, save checkpoints while fine-tuning, run inference tasks on multiple CPUs and much more, I would recommend using the Hugging Face API. A simple tutorial on how to do so was recently released by Hugging Face and can be found here. . In this post, I am not trying to reinvent the wheel, but merely bringing together a list of prexisting excellent resources to make it easier for the reader to grasp GPT-2. I leave it up to the reader to further build upon these foundations in any area they choose. . You can’t build a great building on a weak foundation. You must have a solid foundation if you’re going to have a strong superstructure. . – Gordon B. Hinckley . Prerequisites . This post assumes that the reader has a solid understanding of Attention and Transformers. The GPT-2 utilizes a 12-layer Decoder Only Transformer architecture. If you want a refresher or understand Attention and Transformers, here is an excellent list of resources to aid your understanding regarding: . The illustrated Transformer by Jay Alammar | The Annotated Transformer by Harvard NLP | Introduction to the Transformer by Rachel Thomas and Jeremy Howard | If you’re just beginning your journey into NLP or you’re an expert, I would definitely recommend the fast.ai NLP course taught by Rachel Thomas and Jeremy Howard. The course starts with the basics including Sentiment Classification using Naive Bayes and Logistic Regression, moves on to RNNs and also talks about Transfer Learning, ULMFiT, Seq2Seq translation and Transformers amongst other things. It is an excellent resource put together by the fast.ai team free of cost. . Another amazing resource on GPT-2 itself, is The Illustrated GPT-2 by Jay Alammar. This post starts with a basic introduction to Language Models and explains the GPT-2 model step-by-step in a very easy to understand manner. I would highly recommend the reader to give this post a read. . The Annotated Transformer by Harvard NLP implements the complete Transformer architecture using PyTorch and is great way to understand Attention in depth. . Let’s then build upon these excellent existing resources and implement GPT-2 in code. . . Language Models are Unsupervised Multitask Learners . . Abstract . Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations. . A Zero-shot setting is one where you do not finetune the language model and directly run inference on the target dataset. For example, pretrain a LM on WebText and directly try and predict the next words of Amazon Movie reviews dataset. . Model Architecture (GPT-2) . We use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers. The vocabulary is expanded to 50,257 words. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used. . This is the entirety of model explanation inside the GPT-2 research paper. This warrants a need for us to look at the architecture inside the GPT model. . Model Specifications (GPT) . Our model largely follows the original transformer work. We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in, with w = 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU). . . As can be seen from the GPT Architecture, to implement it, we will first need to implement Masked Self Attention and Feed Forward layer. . Imports . import torch import copy import torch.nn as nn import torch.nn.functional as F from torch.nn.modules import ModuleList from torch.nn.modules.normalization import LayerNorm import numpy as np import os from tqdm import tqdm_notebook, trange import logging logging.basicConfig(level = logging.INFO) logger = logging.getLogger() . Transformer Decoder inside GPT-2 . To re-use the terminology used to describe the Transformer, the attention is a function of a query (Q) and set of key (K) and value (V) pairs. To handle longer sequences, we modify the multi-head self-attention of the Transformer to reduce memory usage by limiting the dot products between Q and K in: . . class Conv1D(nn.Module): def __init__(self, nx, nf): super().__init__() self.nf = nf w = torch.empty(nx, nf) nn.init.normal_(w, std=0.02) self.weight = nn.Parameter(w) self.bias = nn.Parameter(torch.zeros(nf)) def forward(self, x): size_out = x.size()[:-1] + (self.nf,) x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight) x = x.view(*size_out) return x . CONV1D Layer Explained . The CONV1D layer can be thought of as a LINEAR layer itself. Essentially, it is casting an initial tensor x (having the final dimension of x.size(-1)) being passed to it to have a final dimension of size self.nf. . Here’s an example output of the same: . d_model = 768 conv1d = Conv1D(d_model, d_model*3) x = torch.rand(1,4,d_model) #represents a sequence of batch_size=1, seq_len=4 and embedding_sz=768, something like &quot;Hello how are you&quot; x = conv1d(x) x.shape &gt;&gt; torch.Size([1, 4, 2304]) . As can be seen in the example above, the final dimension of tensor returned by CONV1D is 3 times the initial size. We do this to be able to cast the input to query, key and value matrices. . It is possible then to retrieve the query, key and value matrices like so: . query, key, value = x.split(d_model, dim=-1) query.shape, key.shape, value.shape &gt;&gt; (torch.Size([1, 4, 768]), torch.Size([1, 4, 768]), torch.Size([1, 4, 768])) . Another way to cast the input to Q, K and V matrices would have to been to have separate Wq, Wk and Wv matrices. I have explained this under the EXTRA section of this post at the bottom. I find this other approach more intuitive and relatable, but we use the CONV1D layer in this post, because we reuse the CONV1D pretrained weights from Hugging Face. . FEEDFORWARD Layer Explained . class FeedForward(nn.Module): def __init__(self, dropout, d_model=768, nx=768*4): super().__init__() self.c_fc = Conv1D(d_model, nx) self.c_proj = Conv1D(nx, d_model) self.act = F.gelu self.dropout = nn.Dropout(dropout) def forward(self, x): return self.dropout(self.c_proj(self.act(self.c_fc(x)))) . Something, that’s just so well explained in Jay Alammar’s post - also referenced above, is how the inputs are passed through ATTENTION layer first and then on to FEEDFORWARD layer. The Feedforward network, is a normal nueral network that accepts the outputs from the ATTENTION layer (768), casts them to nx (768*4) dimension, adds an activation function self.act (GELU), casts them back to d_model (768) and adds dropout (0.1). . This is also mentioned in the GPT research paper referenced below. . For the position-wise feed-forward networks, we used 3072 dimensional inner states . ATTENTION Layer Explained . The below extract is from the paper Attention is all you need. . Scaled Dot-Product Attention . We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values. . . In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: . . The two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1/√dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/√dk. . To implement the The Attention layer in code, we first utilize the CONV1D layer and get the q, k and v matrices as explained before. . Once we have the q, k and v matrices, we can perform attention using the function _attn. This function replicates the formula mentioned above inside Attention Dot Product. . class Attention(nn.Module): def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False): super().__init__() self.n_head = n_head self.d_model = d_model self.c_attn = Conv1D(d_model, d_model*3) self.scale = scale self.softmax = nn.Softmax(dim=-1) self.register_buffer(&quot;bias&quot;, torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx)) self.dropout = nn.Dropout(0.1) self.c_proj = Conv1D(d_model, d_model) def split_heads(self, x): &quot;return shape [`batch`, `head`, `sequence`, `features`]&quot; new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) x = x.view(*new_shape) return x.permute(0, 2, 1, 3) def _attn(self, q, k, v, attn_mask=None): scores = torch.matmul(q, k.transpose(-2, -1)) if self.scale: scores = scores/math.sqrt(v.size(-1)) nd, ns = scores.size(-2), scores.size(-1) if attn_mask is not None: scores = scores + attn_mask scores = self.softmax(scores) scores = self.dropout(scores) outputs = torch.matmul(scores, v) return outputs def merge_heads(self, x): x = x.permute(0, 2, 1, 3).contiguous() new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),) return x.view(*new_shape) def forward(self, x): x = self.c_attn(x) #new `x` shape - `[1,3,2304]` q, k, v = x.split(self.d_model, dim=2) q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v) out = self._attn(q, k, v) out = self.merge_heads(out) out = self.c_proj(out) return out . Another way to implement Attention is explained in the Extras section at the bottom of this blog. I find it to be more intuitive and easy to compare with the research paper. It utilizes Linear layers instead of CONV1D to cast inputs to Q, K and V matrices. The reason why we haven’t used it is because we use the pretrained weights for CONV1D layer from Hugging Face. . Multi-Head Attention . The below extract is from the paper Attention is all you need. . Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure below. . . Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. . . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. . Not to be confused by this, in essence all that’s being done is to add another dimension to the Q, K and V matrices. That is, if the matrices were before of size [1, 4, 768] which represents [bs, seq_len, d_model], these matrices are projected to dimension [1, 12, 4, 64] which represents [bs, n_head, seq_len, d_model//n_head]. GPT-2 utizes 12 parallel heads. We split the Q, K, V matrices inside split_heads function. Finally, once we get an output from applying parallel attentions we concatenate it inside merge_heads back to matrices of dimension [bs, seq_len, d_model]. . GPT-2 Model Architecture in Code . . So far, we have implemented Multi Head Attention and FeedForward layers. The two layers form the building blocks of the Transformer Decoder block, shown in the picture above. The GPT-2 consists of 12 of these Transformer Blocks. . This has been shown in Jay Alammar’s post like so: . Transformer Decoder Block Explained . class TransformerBlock(nn.Module): def __init__(self, d_model=768, n_head=12, dropout=0.1): super(TransformerBlock, self).__init__() self.attn = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False) self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4) self.ln_1 = LayerNorm(d_model) self.ln_2 = LayerNorm(d_model) def forward(self, x): x = x + self.attn(self.ln_1(x)) x = x + self.feedforward(self.ln_2(x)) return x . The Transformer Block consists of Attention and FeedForward Layers. As referenced from the GPT-2 Architecture Model Specification, . Layer normalization (Ba et al., 2016) was moved to the input of each sub-block Here are the sub-blocks are Attention and FeedForward. . Thus, inside a Transformer Decoder Block, essentially we first pass the inputs to a LayerNorm followed by the first sub-block Attention. Next, we pass the outputs of this sub-block to LayerNorm again and finally to FeedForward layer. . The GPT-2 Architecture Explained . As referenced from the GPT paper, . We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). . Thus, the complete GPT-2 architecture is the TransformerBlock copied over 12 times. . def _get_clones(module, n): return ModuleList([copy.deepcopy(module) for i in range(n)]) class GPT2(nn.Module): def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257): super(GPT2, self).__init__() self.nlayers = nlayers block = TransformerBlock(d_model=768, n_head=12, dropout=0.1) self.h = _get_clones(block, 12) self.wte = nn.Embedding(vcb_sz, d_model) self.wpe = nn.Embedding(n_ctx, d_model) self.drop = nn.Dropout(0.1) self.ln_f = LayerNorm(d_model) self.out = nn.Linear(d_model, vcb_sz, bias=False) self.loss_fn = nn.CrossEntropyLoss() self.init_weights() def init_weights(self): self.out.weight = self.wte.weight self.apply(self._init_weights) def _init_weights(self, module): if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)): module.weight.data.normal_(mean=0.0, std=0.02) if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) def forward(self, src, labels=None, pos_ids=None): if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0) inp = self.drop((self.wte(src)+self.wpe(pos_ids))) for i in range(self.nlayers): inp = self.h[i](inp) inp = self.ln_f(inp) logits = self.out(inp) outputs = (logits,) + (inp,) if labels is not None: shift_logits = logits[..., :-1, :].contiguous() shift_labels = labels[..., 1:].contiguous() loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)) outputs = (loss,) + outputs return outputs return logits . Something I have not mentioned yet is Positional Encoding and Token Embeddings. Since, we cannot pass words such as “hey” or “hello” directly to the model, we first Tokenize our inputs. Next, we use Embeddings to represent the tokens as numbers. This post by Jay Alammar again explains Embeddings very well. . Also, since unlike the RNNs where the input words are passed sequentially, Transformers take input matrices in parallel thus losing the sense of position for the words being input. To make up for the loss, before handling the Token Embeddings to the model, we add Positional Encoding - a signal that indicates the order of the words in the sequence. Since, as mentioned before, the context size of GPT-2 is 1024, the positional encodings are of dimensions [1024, 768]. . . Thus, the inputs to the GPT-2 architecture is the sum of Token Embeddings and Positional Encodings passed through a Dropout, to add regularization. Once, we have the input matrix, we pass this through each of the 12 Layers of the GPT-2 architecure, where each layer is a Transformer Decoder Block that consists of two sublayers - Attention and FeedForward Network. . Language Modeling or Classification . When using GPT-2 as a language model, we pass the inputs to a final LayerNorm and through a Linear layer with a final dimension of size [768, vocab_sz] (50257) and get an output of size [1, 4, 50257]. This output represents the next word logits and we can very easily now pass this through a Softmax layer and take argmax to get the positional of the word inside the vocabulary with the highest probability. . For classification task, we can pass the outputs received from the GPT-2 architecture through a Linear layer with a dimension of size [768, n] to get probabilities for each category (where n represents number of categories), pass it through a softmax, get the highest predicted category and use CrossEntropyLoss to train the architecture to do classification. . And that’s really all the magic behind GPT-2. It’s a Decoder only Transformer Based architecture that takes inputs parallely with Positional Encodings unlike RNNs, passes them through each of it’s 12 Transformer Decoder layers (which consist of Multi head Attention and FeedForward Network) to return the final output. . Let’s see this model in action in a language model task. . Sample text generation using Hugging Face Pretrained Weights . First, let’s initialize the model with the Pretrained Weights already provided by Hugging Face. . model = GPT2() # load pretrained_weights from hugging face # download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin to `.` model_dict = model.state_dict() #currently with random initialization state_dict = torch.load(&quot;./gpt2-pytorch_model.bin&quot;) #pretrained weights old_keys = [] new_keys = [] for key in state_dict.keys(): if &quot;mlp&quot; in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights new_key = key.replace(&quot;mlp&quot;, &quot;feedforward&quot;) new_keys.append(new_key) old_keys.append(key) for old_key, new_key in zip(old_keys, new_keys): state_dict[new_key]=state_dict.pop(old_key) pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict} model_dict.update(pretrained_dict) model.load_state_dict(model_dict) model.eval() #model in inference mode as it&#39;s now initialized with pretrained weights . Let’s now generate text. We will utilize Hugging Face’s pretrained Tokenizer to convert words to input embeddings. . from transformers import GPT2Tokenizer tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;) context = torch.tensor([tokenizer.encode(&quot;The planet earth&quot;)]) def generate(context, ntok=20): for _ in range(ntok): out = model(context) logits = out[:, -1, :] indices_to_remove = logits &lt; torch.topk(logits, 10)[0][..., -1, None] logits[indices_to_remove] = np.NINF next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1) context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1) return context out = generate(context, ntok=20) tokenizer.decode(out[0]) &gt;&gt; &#39;The planet earth is the source of all of all the light,&quot; says the study that the government will&#39; . Extras . Another way to implement Attention as shown in the NLP Course by fast.ai referenced from here, that I find to be more intuitive is as below: . class Attention_FASTAI(nn.Module): def __init__(self, d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False): super().__init__() self.n_head = n_head self.d_head = d_head self.softmax = nn.Softmax(dim=-1) self.scale = scale self.atn_drop = nn.Dropout(0.1) self.wq, self.wk, self.wv = [nn.Linear(d_model, n_head*d_head, bias=bias) for o in range(3)] def split_heads(self, x, layer, bs): x = layer(x) return x.view(bs, x.size(1), self.n_head, self.d_head).permute(0,2,1,3) def _attn(self, q, k, v, attn_mask=None): scores = torch.matmul(q, k.transpose(-2, -1)) if self.scale: scores = scores/math.sqrt(v.size(-1)) if attn_mask is not None: scores = scores.float().masked_fill(attn_mask, -float(&#39;inf&#39;)).type_as(scores) attn_prob = self.atn_drop(self.softmax(scores)) attn_vec = attn_prob @ v return attn_vec def merge_heads(self, x, bs, seq_len): x = x.permute(0, 2, 1, 3).contiguous() return x.view(bs, seq_len, -1) def forward(self, q, k, v, mask=None): bs, seq_len = q.size(0), q.size(1) wq, wk, wv = map(lambda o:self.split_heads(*o, bs), zip((q,k,v), (self.wq, self.wk, self.wv))) attn_vec = self._attn(wq, wk, wv) attn_vec = self.merge_heads(attn_vec, bs, seq_len) return attn_vec . The key difference between the implementation above and the one we have used is that this implementation does not use CONV1D. Instead, we first pass the input x to self.wq, self.wk and self.wv Linear Layers to get wq, wk and wv matrices and then perform attention as before. . Credits . I just want to take the time to thank Rachel Thomas and Jeremy Howard for a great NLP course and the fast.ai course in general, which has helped me bolster my understanding of RNNs, GRUs, AWD-LSTM and Transformers. Also, a special thanks to Hugging Face for creating an open source NLP library and providing a number of Pretrained Models to use. As mentioned the code in this blog post comes directly from the Hugging Face library. And, Jay Alammar for the excellent work that he has been doing to Visualise machine learning concepts. The Illustrated GPT-2 is one of the most comprehensive blog posts on GPT-2. Finally, to Harvard NLP, for The Annotated Transformer, a beautiful and easy to follow implementation of Transformers in PyTorch. . Feedback . Comments or feedback? Please tweet me at @amaarora . This is a really wonderful resource, and draws together many very nice pieces of work. https://t.co/CM16ByNrbt . &mdash; Jeremy Howard (@jeremyphoward) February 19, 2020 Great work pairing GPT2 concepts with the key excerpts from the code. https://t.co/IkFlAf3Ua8 . &mdash; Jay Alammar جهاد العمار (@jalammar) February 20, 2020 &quot;The Annotated GPT-2&quot; blogpost seems to start out a a simple question of asking why use conv-1d vs linear. An awesome read!! https://t.co/GCju0z3Wri #nlproc #nlposs #distiller https://t.co/cvSdEah8gT . &mdash; Liling Tan (@alvations) February 20, 2020 A must-read blog about GPT-2. https://t.co/EuDil5Dm07 . &mdash; Xinhao Li (@XinhaoLi1) February 20, 2020 One of the best NLP Blogposts I&#39;ve read: A definitive and complete writeup. 🍵This is a blog, I wish I had when I was tinkering with the GPT-2. Must read for everyone: https://t.co/yLRFywYgm6 . &mdash; Sanyam Bhutani (@bhutanisanyam1) February 20, 2020 Neat! https://t.co/eLt3o180Qr . &mdash; Antônio Horta Ribeiro (@ahortaribeiro) February 20, 2020 Fantastic work!! Looking forward to learning what is it behind the scenes of this language model! https://t.co/Ml1DY22NxQ . &mdash; Data Enigma (@EnigmaData) February 19, 2020 The Annotated GPT-2 - Understand how the GPT-2 model works underneath with explanations and source codeBlogpost https://t.co/anLqVhZQPN@amaarora𝐬𝐩𝐫𝐞𝐚𝐝 𝐭𝐡𝐞 𝐰𝐨𝐫𝐝 𝐨𝐟 #𝐍𝐋𝐏 💜#datascience #pytorch #deeplearning #machinelearning pic.twitter.com/n5QIQBAIfH . &mdash; Philip Vollet (ﾉ◕ヮ◕)ﾉ*:・ﾟ✧ (@philipvollet) February 20, 2020 This is fantastic @amaarora, thanks 👍 . &mdash; Manpreet Singh (@ms_ghotratweet) February 20, 2020 This is brilliant stuff!!! . &mdash; Arron Hovingham (@AnalystiaArron) February 20, 2020",
            "url": "https://amaarora.github.io/fastexplain/2020/02/18/annotatedGPT2.html",
            "relUrl": "/2020/02/18/annotatedGPT2.html",
            "date": " • Feb 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Data Scientist at CoreLogic Australia where I spend a lot of my time building SOTA algorithms to predict house and rental prices across all of Australia and NZ. At CoreLogic, I have been responsible for building the Rental AVM product from scratch for over 10 million properties which led to an overall 8% increase in predictions nationally. . I have a passion for deep learning, thanks to Jeremy Howard who made it possible for me through fast.ai to begin my journey in this space around 1.5 years ago. . I am self taught, and an ardent follower of and small contributor to the fast.ai course. Through this course itself, I have been able to pickup Python and it has made possible for me to do my own research. Something, I would have not thought of being able to do when I first started. I believe I still have a lot to learn and commit myself to learning something new every day. . I also fimly believe in giving back to the community and in that attempt, also run a meetup group called Applied Data Science. The focus in this blog is on writing code as much as it is on explaining the concept in theory. . If you feel that any of these ideas can be expressed in a better way, I am very happy to receive constructive feedback. Please feel free to reach out to me via twitter. .",
          "url": "https://amaarora.github.io/fastexplain/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://amaarora.github.io/fastexplain/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}